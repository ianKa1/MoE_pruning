{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5b0243-2043-4951-9239-d817a2036dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67706caef50443ea97361d81d90fe316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b3e71b-1550-4872-8ee8-fb61347126a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ddaec5573547ceb4c5ae1e5b27ce6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm a 20 year old girl from the Netherlands. I'm a student and I'm\n",
      "trainable params: 62,133,888 || all params: 14,377,918,080 || trainable%: 0.4321\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-MoE-A2.7B\"\n",
    "\n",
    "# 4-bit QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                 # QLoRA = 4-bit base model\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",         # QLoRA uses nf4\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "inputs = tokenizer(\"Hello!\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from peft import IA3Config, get_peft_model, TaskType\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(r = 4,\n",
    "                    lora_alpha=4,\n",
    "                    target_modules = [\"gate\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "                    lora_dropout=0.1\n",
    "                    )\n",
    "\n",
    "lora_model = get_peft_model(model, config)\n",
    "\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5fbb256-3e79-4b12-81b1-3ff50b9c411f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d5ff0cec864e328580b3dfcccd2a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c73aaf3c7c349dfac1e734409fcafec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1c1171a5ea484a9d3987f968e19b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"Na0s/sft-ready-Text-Generation-Augmented-Data\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a0b1f0-5725-44ec-81a9-a7173975c657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 7667416\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e748a2-56a9-4f17-9384-182b7a862b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(batch):\n",
    "#     texts = []\n",
    "#     for prompt, completion in zip(batch[\"prompt\"], batch[\"completion\"]):\n",
    "#         text = tokenizer.apply_chat_template(\n",
    "#             [\n",
    "#                 {\"role\": \"user\", \"content\": prompt},\n",
    "#                 {\"role\": \"assistant\", \"content\": completion},\n",
    "#             ],\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=False,\n",
    "#         )\n",
    "\n",
    "#         if isinstance(text, list):\n",
    "#             text = \"\".join(text)\n",
    "\n",
    "#         texts.append(text)\n",
    "\n",
    "#     return tokenizer(\n",
    "#         texts,\n",
    "#         truncation=True,\n",
    "#         padding=False,\n",
    "#         max_length=2048\n",
    "#     )\n",
    "\n",
    "# batch = dataset.select(range(3))\n",
    "# tokens = tokenize(batch)\n",
    "# tokenizer.decode(tokens[\"input_ids\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9966c92f-99e8-40ba-a57e-122de840f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset = dataset.map(\n",
    "#     tokenize,\n",
    "#     batched=True,\n",
    "#     num_proc=75,\n",
    "#     remove_columns=[\"prompt\", \"completion\"],\n",
    "# )\n",
    "\n",
    "# print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971b55c8-340e-498d-bd1d-34996979b355",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     11\u001b[39m         new_attention.append(mask)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: new_input_ids,\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m: new_attention,\n\u001b[32m     16\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m tokenized_dataset = \u001b[43mtokenized_dataset\u001b[49m.map(add_eos, batched=\u001b[38;5;28;01mTrue\u001b[39;00m, num_proc=\u001b[32m75\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ff0f19-f195-4a88-bc54-a083ed5cd750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30af5b8919854e868b5b31fb0d65f0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143911b29aab4366873b828b5ffce908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788e146318dd4f659dedb94386832508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "tokenized_dataset = load_dataset(\"kaaiiii/tokenized_data_for_MoE\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4971018e-65c4-48f0-951a-eab7cce93fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = tokenizer.eos_token_id  # Qwen: tokenizer.eos_token_id\n",
    "\n",
    "def add_eos(batch):\n",
    "    new_input_ids = []\n",
    "    new_attention = []\n",
    "\n",
    "    for ids, mask in zip(batch[\"input_ids\"], batch[\"attention_mask\"]):\n",
    "        ids = ids + [EOS]\n",
    "        mask = mask + [1]\n",
    "        new_input_ids.append(ids)\n",
    "        new_attention.append(mask)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": new_input_ids,\n",
    "        \"attention_mask\": new_attention,\n",
    "    }\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_eos, batched=True, num_proc=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171cce6-7e3e-4a54-902b-9c410caf3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenized_dataset[0:5])\n",
    "# tokenized_dataset.save_to_disk(\"./tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "340f3fd7-a707-4596-9c42-c42b8ec295ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\n0.002 = 1000 \\n1 = x?<|im_end|>\\n<|im_start|>assistant\\nTo find the value of x, we can set up a proportion using the given information:\\n\\n0.002/1000 = 1/x\\n\\nTo solve for x, we can cross multiply:\\n\\n0.002 * x = 1000 * 1\\n\\n0.002x = 1000\\n\\nDividing both sides by 0.002:\\n\\nx = 1000 / 0.002\\n\\nx = 500,000\\n\\nTherefore, 1 is equal to 500,000 in this proportion.<|im_end|>\\n<|endoftext|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.decode(tokenized_dataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c672b3-e252-409f-99ef-72548a3fff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1007' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1007/2000 19:57:21 < 19:43:03, 0.01 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.789100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.810600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.823100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.891400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.806200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.806300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.881300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.780300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.781600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.876100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.779200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.817600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.832600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.824700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.774700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.847800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.821300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.760600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.724100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.817400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.765800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.712300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.712900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.696400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.782200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.720800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.657500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.821100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.760200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.657300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.660300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.744700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.757200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.651300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.679700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.706700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.636100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.680800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.660300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.606700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.632500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.670900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.548600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.622900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.554300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.572300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.534400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.582400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.567200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.547800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.507200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.563100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.627800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.532100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.666600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.593800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.574400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.642400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.480100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.544900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.520300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # causal LM\n",
    ")\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "lora_model.enable_input_require_grads()\n",
    "lora_model.gradient_checkpointing_enable()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs_endoftext\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8, \n",
    "    max_steps=2000,               # try a small number first\n",
    "\n",
    "    dataloader_num_workers = 4,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_prefetch_factor=4,\n",
    "    dataloader_persistent_workers=True,\n",
    "    \n",
    "    learning_rate=1e-6,\n",
    "    weight_decay=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True,\n",
    "\n",
    "    logging_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    disable_tqdm=False,    # ensure progress bar is visible\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=10,\n",
    "    save_safetensors=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"kaaiiii/QwenA2.7B_endoftext\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,          # your LoRA-wrapped model\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(\"Qwen1.5_QwenA2.7B_endoftext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8980b-6135-453c-9066-8a802bc94ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.push_to_hub(\"kaaiiii/Qwen1.5b_MoE_LoRA_mode_500_steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7853727-3f90-4d89-b443-bcc844e5f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trl import SFTConfig\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B\", use_fast=True)\n",
    "# print(lora_model.device)\n",
    "\n",
    "# lora_model.enable_input_require_grads()\n",
    "# lora_model.gradient_checkpointing_enable()\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model = lora_model,\n",
    "#     train_dataset = dataset,\n",
    "#     processing_class = tokenizer,\n",
    "#     args = SFTConfig(\n",
    "#         per_device_train_batch_size = 4,\n",
    "#         gradient_accumulation_steps = 8,\n",
    "\n",
    "#         dataloader_num_workers = 8,\n",
    "#         dataloader_pin_memory=True,\n",
    "#         dataloader_prefetch_factor=4,\n",
    "#         dataloader_persistent_workers=True,\n",
    "        \n",
    "#         packing = True,\n",
    "#         group_by_length = True,\n",
    "#         warmup_ratio = 0.05,\n",
    "#         bf16 = True,\n",
    "#         max_steps=1000,\n",
    "#         learning_rate = 5e-6,\n",
    "#         optim = \"adamw_8bit\",\n",
    "#         weight_decay = 0.03,\n",
    "#         lr_scheduler_type = \"cosine\",\n",
    "#         seed = 423,\n",
    "#         eval_strategy=\"no\",\n",
    "#         do_eval=False,\n",
    "#         output_dir = \"./outputs_SFT\",\n",
    "\n",
    "#         save_strategy=\"steps\",\n",
    "#         save_steps=500,\n",
    "#         save_total_limit=10,\n",
    "#         save_safetensors=True,\n",
    "#         push_to_hub=True,   \n",
    "#         hub_model_id=\"kaaiiii/Qwen1.5_MoE_2000steps\",\n",
    "        \n",
    "#         remove_unused_columns=False,\n",
    "#         logging_steps = 10,\n",
    "#         disable_tqdm=False,\n",
    "#         report_to=\"none\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# trainer.model.save_pretrained(\"Qwen1.5_MoE_lora_model_lr5e_6_1000steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199aec6-9301-4df1-94e9-76592d64ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94fc05b-37d5-4813-a0f1-c1b668bbd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2c803-6a53-4280-84ce-261299c8ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7737751-fa15-4d58-bca0-23c8079edcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_router_layers(model):\n",
    "    router_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if \"mlp.gate\" in name or \"mlp.shared_expert_gate\" in name:\n",
    "            router_layers.append(name)\n",
    "    return router_layers\n",
    "\n",
    "routers = find_all_router_layers(model)\n",
    "for r in routers:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02f656-3dd5-4591-a391-7bfdfc021864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_router_l2_norms(model):\n",
    "    layer_norms = {}\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if \"mlp.gate\" in name and hasattr(module, \"base_layer\"):\n",
    "            weight = module.base_layer.weight  # (60, 2048)\n",
    "            norms = torch.norm(weight.float(), dim=1)  # shape (60,)\n",
    "            layer_norms[name] = norms.detach().cpu()\n",
    "\n",
    "    return layer_norms\n",
    "\n",
    "router_norms = get_router_l2_norms(model)\n",
    "\n",
    "for name, norms in list(router_norms.items())[:3]:\n",
    "    print(norms.shape)\n",
    "    print(name, norms[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58938b97-3f62-48f1-b367-5a995a43bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(router_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf282c5-1d18-49a1-8df6-01fb004c1ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_all_layers(norms):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    for i, (name, norm) in enumerate(norms.items()):\n",
    "        plt.plot(norm, label=f\"L{i}\", alpha=0.5)\n",
    "\n",
    "    plt.title(\"Router Expert L2 Norms Across All Layers\")\n",
    "    plt.xlabel(\"Expert Index (0-59)\")\n",
    "    plt.ylabel(\"L2 Norm\")\n",
    "    plt.legend(ncol=3, fontsize=7)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_all_layers(router_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0560a5f5-a713-4119-abcc-988b6185bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_experts_to_prune(router_norms, prune_ratio=0.1):\n",
    "    prune_indices = {}\n",
    "\n",
    "    for layer_name, norms in router_norms.items():\n",
    "        k = int(len(norms) * prune_ratio)\n",
    "        prune_idx = torch.topk(norms, k=k, largest=False).indices\n",
    "        prune_indices[layer_name] = prune_idx.tolist()\n",
    "\n",
    "    return prune_indices\n",
    "\n",
    "prune_plan = choose_experts_to_prune(router_norms, prune_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9682e133-8507-4928-93e0-2a90a53dae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_moe_layer(mlp, prune_idx):\n",
    "    prune_idx = sorted(prune_idx)\n",
    "\n",
    "    # 1. prune gate\n",
    "    old_w = mlp.gate.base_layer.weight  # (60, 2048)\n",
    "    keep_idx = [i for i in range(old_w.size(0)) if i not in prune_idx]\n",
    "\n",
    "    new_gate_weight = old_w[keep_idx, :].clone()\n",
    "    mlp.gate.base_layer.weight = torch.nn.Parameter(new_gate_weight)\n",
    "\n",
    "    # 2. prune experts\n",
    "    new_experts = torch.nn.ModuleList([\n",
    "        exp for i, exp in enumerate(mlp.experts)\n",
    "        if i not in prune_idx\n",
    "    ])\n",
    "    mlp.experts = new_experts\n",
    "\n",
    "    print(f\"Pruned {len(prune_idx)} experts, left {len(mlp.experts)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a3815-580b-4470-ba67-df2b4dc5bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, prune_plan):\n",
    "    for name, module in model.named_modules():\n",
    "        if \"mlp\" in name and hasattr(module, \"experts\"):\n",
    "            if name + \".gate\" in prune_plan:\n",
    "                prune_idx = prune_plan[name + \".gate\"]\n",
    "                prune_moe_layer(module, prune_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
