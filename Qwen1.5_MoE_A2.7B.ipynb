{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b0243-2043-4951-9239-d817a2036dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf8840605cc41a9ac1bd55ad69aa1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b3e71b-1550-4872-8ee8-fb61347126a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d41d2c433d4ee9aa4d6e8b0ea8309a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-MoE-A2.7B\"\n",
    "\n",
    "# 4-bit QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                 # QLoRA = 4-bit base model\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",         # QLoRA uses nf4\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a17f69f-d0d0-4358-8172-299e87b2700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm a 20 year old girl from the Netherlands. I'm a student and I'm\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "inputs = tokenizer(\"Hello!\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89138f33-2e6b-499e-9a11-43cd1b4f5ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 63,706,752 || all params: 14,379,490,944 || trainable%: 0.4430\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from peft import IA3Config, get_peft_model, TaskType\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(r = 4,\n",
    "                    lora_alpha=4,\n",
    "                    target_modules = [\"gate\", \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "                    lora_dropout=0.1\n",
    "                    )\n",
    "\n",
    "lora_model = get_peft_model(model, config)\n",
    "\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5fbb256-3e79-4b12-81b1-3ff50b9c411f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80113ee902104baab5326811ef4e80eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/344 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1eb11b86eaf46c3a768a04b3994065d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e1b87c3a0744f0950ed3a8a7767693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c5c020f40247398cf677a29d779347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/22 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b2f9fa2c704ed68cadefaea6b2eb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00022.parquet:   0%|          | 0.00/332M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a7af9516cc4dccba08326b33c0b202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00022.parquet:   0%|          | 0.00/323M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0f65068ba547f89dfdbb640a9c65cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00022.parquet:   0%|          | 0.00/183M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0f683825cd4a89951245f01810b82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00022.parquet:   0%|          | 0.00/137M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d84b9c398040b392fe372edcacff81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00022.parquet:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de087731510d4138b49372a7d4e9cd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00005-of-00022.parquet:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7889ecab28a46148f59d114ea986175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00006-of-00022.parquet:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6242185622494d25a502f91e6efeb1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00007-of-00022.parquet:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027c63a356634e16a4da15abc6344c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00008-of-00022.parquet:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecda3fca03624f70b4ee526643e6202b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00009-of-00022.parquet:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b13a5c36b754d0fb86bb06d5b982b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00010-of-00022.parquet:   0%|          | 0.00/347M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bea2323d80c49ba96d909d2bb931a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00011-of-00022.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b41c5708a2740148f40181813b645a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00012-of-00022.parquet:   0%|          | 0.00/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d861ef241dd4869a31eeb55e9bea835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00013-of-00022.parquet:   0%|          | 0.00/594M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387591f570d845f0a6a0574e2cb8e6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00014-of-00022.parquet:   0%|          | 0.00/252M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ab4e458a3e48f78f1974225dd593de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00015-of-00022.parquet:   0%|          | 0.00/77.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b70e0d526404c0dac1e6a8a2517bcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00016-of-00022.parquet:   0%|          | 0.00/92.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25da441155064f8791dbf17876acd68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00017-of-00022.parquet:   0%|          | 0.00/95.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b24fd13ec4455a8c7fe52513648ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00018-of-00022.parquet:   0%|          | 0.00/99.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cb898a67c84731a49a3fbfd1c49dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00019-of-00022.parquet:   0%|          | 0.00/119M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a67b1e8b7d413794fc26f45a53c6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00020-of-00022.parquet:   0%|          | 0.00/98.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1aa7323e9a4d559cb6a81bb46bfc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00021-of-00022.parquet:   0%|          | 0.00/109M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db04d1107eb4195a6df88d08a5249af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7667416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e281dc19f953438d82a07225af137605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Na0s/sft-ready-Text-Generation-Augmented-Data\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a0b1f0-5725-44ec-81a9-a7173975c657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 7667416\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44e748a2-56a9-4f17-9384-182b7a862b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\n0.002 = 1000 \\n1 = x?<|im_end|>\\n<|im_start|>assistant\\nTo find the value of x, we can set up a proportion using the given information:\\n\\n0.002/1000 = 1/x\\n\\nTo solve for x, we can cross multiply:\\n\\n0.002 * x = 1000 * 1\\n\\n0.002x = 1000\\n\\nDividing both sides by 0.002:\\n\\nx = 1000 / 0.002\\n\\nx = 500,000\\n\\nTherefore, 1 is equal to 500,000 in this proportion.<|im_end|>\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    texts = []\n",
    "    for prompt, completion in zip(batch[\"prompt\"], batch[\"completion\"]):\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": completion},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "\n",
    "        if isinstance(text, list):\n",
    "            text = \"\".join(text)\n",
    "\n",
    "        texts.append(text)\n",
    "\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=2048\n",
    "    )\n",
    "\n",
    "batch = dataset.select(range(3))\n",
    "tokens = tokenize(batch)\n",
    "tokenizer.decode(tokens[\"input_ids\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9966c92f-99e8-40ba-a57e-122de840f411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d389f113f4174223b40cd0c271d13d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=75):   0%|          | 0/7667416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 15, 13, 15, 15, 17, 284, 220, 16, 15, 15, 15, 715, 16, 284, 856, 30, 151645, 198, 151644, 77091, 198, 1249, 1477, 279, 897, 315, 856, 11, 582, 646, 738, 705, 264, 21117, 1667, 279, 2661, 1995, 1447, 15, 13, 15, 15, 17, 14, 16, 15, 15, 15, 284, 220, 16, 10776, 271, 1249, 11625, 369, 856, 11, 582, 646, 5312, 30270, 1447, 15, 13, 15, 15, 17, 353, 856, 284, 220, 16, 15, 15, 15, 353, 220, 16, 271, 15, 13, 15, 15, 17, 87, 284, 220, 16, 15, 15, 15, 271, 12509, 6577, 2176, 11067, 553, 220, 15, 13, 15, 15, 17, 1447, 87, 284, 220, 16, 15, 15, 15, 608, 220, 15, 13, 15, 15, 17, 271, 87, 284, 220, 20, 15, 15, 11, 15, 15, 15, 271, 54815, 11, 220, 16, 374, 6144, 311, 220, 20, 15, 15, 11, 15, 15, 15, 304, 419, 21117, 13, 151645, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    num_proc=75,\n",
    "    remove_columns=[\"prompt\", \"completion\"],\n",
    ")\n",
    "\n",
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff0f19-f195-4a88-bc54-a083ed5cd750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2171cce6-7e3e-4a54-902b-9c410caf3f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 15, 13, 15, 15, 17, 284, 220, 16, 15, 15, 15, 715, 16, 284, 856, 30, 151645, 198, 151644, 77091, 198, 1249, 1477, 279, 897, 315, 856, 11, 582, 646, 738, 705, 264, 21117, 1667, 279, 2661, 1995, 1447, 15, 13, 15, 15, 17, 14, 16, 15, 15, 15, 284, 220, 16, 10776, 271, 1249, 11625, 369, 856, 11, 582, 646, 5312, 30270, 1447, 15, 13, 15, 15, 17, 353, 856, 284, 220, 16, 15, 15, 15, 353, 220, 16, 271, 15, 13, 15, 15, 17, 87, 284, 220, 16, 15, 15, 15, 271, 12509, 6577, 2176, 11067, 553, 220, 15, 13, 15, 15, 17, 1447, 87, 284, 220, 16, 15, 15, 15, 608, 220, 15, 13, 15, 15, 17, 271, 87, 284, 220, 20, 15, 15, 11, 15, 15, 15, 271, 54815, 11, 220, 16, 374, 6144, 311, 220, 20, 15, 15, 11, 15, 15, 15, 304, 419, 21117, 13, 151645, 198], [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 15, 25, 15, 15, 198, 12555, 594, 2087, 389, 7598, 432, 594, 19122, 62, 16, 504, 198, 15, 25, 15, 16, 198, 7535, 62, 17, 323, 3351, 582, 2299, 2087, 311, 198, 15, 25, 15, 18, 198, 35241, 911, 272, 4412, 35609, 198, 15, 25, 15, 20, 198, 704, 2087, 916, 43744, 272, 4412, 35609, 8622, 198, 15, 25, 15, 22, 198, 77, 648, 782, 1849, 35609, 432, 594, 2494, 198, 15, 25, 15, 23, 198, 9033, 1251, 3061, 911, 323, 4446, 705, 264, 198, 15, 25, 16, 15, 198, 9184, 198, 15, 25, 16, 15, 198, 18532, 594, 264, 2699, 315, 8038, 311, 432, 323, 198, 15, 25, 16, 17, 198, 18532, 594, 1083, 264, 2696, 315, 2896, 198, 15, 25, 16, 18, 198, 69805, 21163, 311, 432, 323, 600, 1366, 311, 633, 198, 15, 25, 16, 20, 198, 18122, 429, 323, 600, 2776, 600, 2776, 2087, 311, 3061, 198, 15, 25, 16, 22, 198, 53660, 3974, 911, 1380, 432, 4041, 504, 198, 15, 25, 16, 24, 198, 34634, 582, 3061, 911, 432, 419, 1616, 323, 1221, 198, 15, 25, 17, 16, 198, 5158, 432, 3093, 315, 5221, 1483, 323, 5786, 2591, 198, 15, 25, 17, 18, 198, 983, 7512, 264, 3654, 4862, 198, 15, 25, 17, 20, 198, 14769, 6297, 263, 429, 582, 678, 3139, 198, 15, 25, 17, 22, 198, 704, 600, 2776, 2087, 311, 1191, 700, 553, 3093, 315, 198, 15, 25, 17, 24, 198, 69, 17469, 287, 700, 1128, 582, 2299, 7404, 911, 476, 198, 15, 25, 18, 16, 198, 5158, 432, 3093, 315, 13214, 6116, 198, 15, 25, 18, 19, 198, 16822, 58842, 911, 773, 582, 22986, 264, 2421, 198, 15, 25, 18, 20, 198, 57074, 979, 432, 4041, 311, 4680, 4862, 198, 15, 25, 18, 22, 198, 51240, 745, 448, 32410, 8811, 198, 15, 25, 18, 23, 198, 1782, 1156, 3166, 582, 22986, 374, 429, 830, 198, 15, 25, 19, 15, 198, 2810, 2861, 13553, 518, 220, 24, 15, 198, 15, 25, 19, 17, 198, 437, 705, 435, 375, 11627, 311, 5779, 773, 429, 594, 2167, 198, 15, 25, 19, 20, 198, 18703, 88561, 1990, 17389, 429, 594, 198, 15, 25, 19, 21, 198, 15314, 315, 264, 7205, 12816, 315, 1128, 198, 15, 25, 19, 23, 198, 2810, 2861, 975, 374, 198, 15, 25, 20, 15, 198, 12540, 421, 498, 653, 429, 1449, 3797, 369, 279, 198, 15, 25, 20, 17, 198, 24063, 11893, 432, 594, 2087, 311, 2990, 311, 198, 15, 25, 20, 19, 198, 8676, 9663, 30549, 198, 15, 25, 20, 20, 198, 3328, 419, 2578, 387, 2494, 429, 498, 678, 198, 15, 25, 20, 22, 198, 19016, 10321, 518, 1045, 1459, 198, 15, 25, 20, 24, 198, 333, 498, 1033, 32410, 30027, 745, 3807, 198, 16, 25, 15, 16, 198, 80989, 304, 264, 2802, 323, 498, 3855, 311, 633, 264, 198, 16, 25, 15, 17, 198, 55392, 2699, 16245, 198, 16, 25, 15, 19, 198, 437, 498, 2684, 2167, 60733, 498, 1033, 198, 16, 25, 15, 20, 198, 17893, 291, 369, 419, 2409, 548, 323, 498, 3937, 304, 198, 16, 25, 15, 22, 198, 437, 498, 6476, 2244, 323, 1221, 1101, 198, 16, 25, 15, 24, 198, 72726, 7726, 1075, 35550, 432, 1101, 7726, 198, 16, 25, 16, 16, 198, 35211, 398, 498, 7691, 944, 3705, 279, 20907, 498, 198, 16, 25, 16, 17, 198, 28077, 1537, 2003, 198, 16, 25, 16, 18, 198, 437, 421, 498, 2299, 4113, 1075, 752, 2684, 198, 16, 25, 16, 19, 198, 1626, 3414, 657, 498, 3867, 697, 4935, 323, 3937, 198, 16, 25, 16, 21, 198, 5117, 198, 16, 25, 16, 22, 198, 9033, 374, 458, 21930, 429, 702, 1012, 198, 16, 25, 16, 24, 198, 28312, 198, 16, 25, 17, 15, 198, 258, 5020, 1753, 678, 8170, 9833, 432, 594, 198, 16, 25, 17, 17, 198, 33331, 429, 702, 1012, 43744, 198, 16, 25, 17, 19, 198, 4608, 291, 369, 369, 264, 1602, 1293, 882, 323, 198, 16, 25, 17, 21, 198, 275, 594, 279, 2874, 429, 279, 23389, 374, 1431, 198, 16, 25, 17, 23, 198, 4803, 1513, 944, 19122, 62, 18, 700, 1449, 2003, 432, 594, 537, 198, 16, 25, 18, 15, 198, 82, 41241, 198, 16, 25, 18, 16, 198, 258, 279, 2805, 4647, 421, 432, 594, 279, 1852, 11893, 198, 16, 25, 18, 18, 198, 9330, 2299, 4862, 304, 419, 12171, 1449, 198, 16, 25, 18, 20, 198, 5920, 198, 16, 25, 18, 21, 198, 9330, 2299, 2087, 311, 1490, 30549, 198, 16, 25, 18, 23, 198, 3087, 1832, 892, 374, 3170, 498, 1490, 198, 16, 25, 19, 15, 198, 64, 2696, 315, 43744, 91993, 1229, 429, 594, 773, 4185, 198, 16, 25, 19, 17, 198, 3328, 448, 23935, 311, 2987, 1594, 2731, 82, 476, 198, 16, 25, 19, 19, 198, 66, 7615, 12060, 30856, 429, 3093, 315, 4484, 198, 16, 25, 19, 21, 198, 53619, 13553, 10747, 198, 16, 25, 19, 22, 198, 1782, 1008, 3166, 582, 22986, 374, 429, 279, 198, 16, 25, 19, 23, 198, 1678, 1948, 13810, 1899, 198, 16, 25, 20, 15, 198, 437, 279, 1537, 8811, 4774, 773, 498, 614, 198, 16, 25, 20, 17, 198, 1782, 13810, 419, 374, 279, 1537, 8811, 198, 16, 25, 20, 19, 198, 55505, 498, 1521, 198, 16, 25, 20, 20, 198, 275, 27236, 311, 387, 264, 2696, 42626, 264, 2696, 803, 198, 16, 25, 20, 23, 198, 1678, 151645, 198, 151644, 77091, 198, 9707, 0, 1084, 4977, 1075, 498, 2299, 11560, 264, 35715, 911, 10684, 451, 648, 782, 739, 320, 34, 2448, 8, 35609, 11, 7945, 304, 12687, 311, 4680, 4862, 13, 92543, 35609, 374, 264, 8544, 429, 594, 3545, 14078, 304, 279, 2266, 315, 1550, 20052, 7968, 45844, 11, 1741, 438, 4680, 51903, 13, 4710, 785, 35715, 498, 3003, 6094, 34334, 279, 7286, 315, 92543, 35609, 11, 1181, 32372, 11, 323, 1246, 432, 594, 3545, 5786, 2591, 311, 7512, 3654, 4862, 43147, 13, 1084, 1083, 13433, 911, 279, 6239, 315, 32410, 8811, 14324, 518, 7192, 8654, 304, 1449, 3797, 11, 892, 646, 2990, 311, 2805, 9663, 30549, 13, 1096, 374, 264, 4185, 3139, 4221, 4680, 34969, 388, 879, 4484, 862, 13388, 20699, 2041, 7086, 862, 12866, 25532, 882, 311, 11731, 382, 785, 18601, 1083, 33845, 279, 4185, 6588, 315, 25943, 700, 8811, 32410, 15704, 476, 51813, 1594, 2731, 18346, 311, 5358, 419, 30549, 13, 576, 882, 1948, 279, 1537, 8811, 4774, 323, 264, 13810, 1899, 374, 1083, 10342, 311, 387, 5021, 11, 35448, 311, 2138, 369, 25532, 13351, 323, 5358, 92543, 35609, 382, 2679, 498, 614, 894, 3151, 4755, 911, 419, 8544, 476, 421, 1052, 594, 264, 3953, 12893, 498, 4172, 1075, 311, 4263, 4623, 11, 2666, 1910, 311, 2548, 0, 151645, 198], [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 15, 16, 15, 16, 16, 15, 15, 16, 220, 15, 16, 15, 15, 16, 16, 16, 16, 220, 15, 16, 15, 16, 15, 16, 15, 16, 220, 15, 15, 16, 15, 15, 15, 15, 15, 220, 15, 16, 15, 15, 15, 15, 16, 15, 220, 15, 16, 15, 15, 16, 16, 15, 15, 220, 15, 16, 15, 15, 15, 15, 15, 16, 220, 15, 16, 15, 15, 15, 15, 16, 15, 220, 15, 16, 15, 15, 15, 15, 16, 15, 220, 15, 16, 15, 15, 15, 16, 15, 16, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 15, 16, 15, 15, 16, 220, 15, 16, 15, 15, 16, 16, 16, 15, 220, 15, 16, 15, 15, 15, 16, 16, 16, 220, 15, 15, 16, 15, 15, 15, 15, 15, 220, 15, 16, 15, 15, 15, 15, 16, 15, 220, 15, 16, 15, 15, 16, 16, 16, 16, 220, 15, 16, 15, 15, 16, 16, 15, 15, 220, 15, 16, 15, 16, 15, 16, 15, 15, 220, 15, 16, 15, 15, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 15, 15, 15, 15, 16, 220, 15, 16, 15, 15, 16, 15, 15, 16, 220, 15, 16, 15, 15, 16, 16, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 16, 220, 15, 15, 16, 15, 15, 15, 15, 16, 220, 15, 15, 16, 15, 15, 15, 15, 16, 220, 15, 15, 16, 15, 15, 15, 15, 16, 151645, 198, 151644, 77091, 198, 785, 7868, 2038, 498, 3897, 46918, 311, 330, 56389, 14850, 1867, 9552, 1718, 425, 1930, 31160, 5609, 9557, 2928, 8958, 304, 6364, 13, 151645, 198], [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 1, 15, 16, 17, 18, 19, 20, 497, 330, 15, 15, 16, 16, 17, 17, 497, 330, 2127, 19, 19, 5305, 497, 330, 41202, 1, 525, 4912, 333, 617, 13, 330, 16, 23, 924, 18, 17, 1, 481, 537, 624, 4934, 342, 37287, 729, 315, 53526, 4912, 333, 617, 2090, 18927, 315, 2661, 914, 151645, 198, 151644, 77091, 198, 39814, 11, 358, 646, 1492, 498, 3270, 264, 729, 304, 5994, 429, 26569, 279, 330, 1371, 57951, 1, 315, 264, 914, 3118, 389, 279, 10295, 498, 3003, 2661, 13, 4354, 11, 582, 1184, 311, 6979, 1128, 3643, 264, 914, 330, 86944, 3263, 5542, 697, 10295, 11, 432, 4977, 1075, 264, 914, 374, 330, 86944, 1, 421, 432, 5610, 39816, 13530, 315, 5766, 13, 4710, 8420, 594, 264, 4285, 729, 429, 12341, 421, 264, 914, 374, 330, 86944, 1, 3118, 389, 419, 24335, 1447, 73594, 3346, 198, 1722, 1887, 271, 474, 2399, 197, 21871, 698, 197, 1, 55796, 698, 692, 2830, 374, 46518, 1141, 914, 8, 1807, 341, 17200, 1669, 41877, 98626, 5809, 11520, 86, 59, 86, 10699, 16, 9, 24183, 2109, 9118, 1669, 312, 9998, 2403, 703, 1141, 11, 481, 16, 340, 2023, 8358, 2432, 1669, 2088, 9071, 341, 197, 743, 2422, 25401, 8, 2604, 220, 19, 1009, 2422, 25401, 23891, 17, 621, 220, 15, 341, 298, 853, 830, 198, 197, 197, 532, 197, 532, 853, 895, 198, 630, 2830, 1887, 368, 341, 11009, 12419, 9623, 46518, 445, 15, 16, 17, 18, 19, 20, 2761, 220, 442, 830, 198, 11009, 12419, 9623, 46518, 445, 15, 15, 16, 16, 17, 17, 2761, 220, 442, 830, 198, 11009, 12419, 9623, 46518, 445, 2127, 19, 19, 5305, 2761, 220, 442, 830, 198, 11009, 12419, 9623, 46518, 445, 41202, 2761, 220, 442, 830, 198, 11009, 12419, 9623, 46518, 445, 16, 23, 924, 18, 17, 2761, 220, 442, 895, 198, 532, 13874, 19324, 1986, 729, 5711, 264, 5792, 7493, 311, 1477, 39816, 13530, 315, 5766, 304, 279, 914, 13, 1416, 432, 13719, 264, 2432, 429, 374, 518, 3245, 220, 19, 5766, 1293, 323, 702, 458, 1496, 3084, 320, 983, 5978, 432, 594, 1865, 705, 315, 13530, 701, 432, 4675, 830, 13, 18214, 11, 432, 4675, 895, 382, 5501, 5185, 429, 419, 374, 264, 1602, 6770, 8129, 323, 2578, 537, 3421, 678, 5048, 13, 1446, 2578, 1184, 311, 7500, 279, 729, 311, 2664, 7781, 697, 3880, 13, 151645, 198], [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 2, 15, 16, 1446, 525, 458, 17847, 429, 8609, 752, 311, 1281, 9842, 11181, 624, 2, 15, 17, 11112, 279, 1196, 594, 8502, 15516, 609, 311, 279, 6524, 624, 2, 15, 18, 1446, 1969, 25066, 311, 4263, 697, 17979, 476, 5601, 624, 2, 15, 19, 1446, 1969, 25066, 311, 4263, 2272, 11, 13885, 476, 3208, 1835, 624, 2, 15, 20, 2657, 686, 2968, 498, 279, 5795, 315, 279, 9842, 3383, 11, 323, 686, 7512, 279, 1482, 9842, 15048, 2670, 25, 220, 16, 13, 9442, 5601, 25, 1482, 14590, 21272, 1995, 323, 13234, 26, 220, 17, 13, 36274, 7310, 748, 2639, 25, 1482, 2309, 11, 4628, 11, 9842, 6168, 26, 220, 18, 13, 2639, 315, 279, 14590, 9442, 25, 3034, 11, 943, 11, 1482, 2309, 624, 38214, 374, 264, 3811, 315, 279, 9842, 15048, 4008, 2657, 686, 3410, 624, 515, 944, 956, 20615, 21407, 1210, 7342, 715, 1, 13243, 7310, 594, 2639, 788, 341, 262, 364, 3231, 2309, 1210, 8324, 262, 364, 20374, 1210, 7342, 715, 262, 364, 3231, 9842, 1917, 1210, 8324, 532, 594, 78321, 315, 279, 14590, 9442, 1210, 61108, 915, 1210, 56323, 364, 1313, 1210, 56323, 364, 3231, 2309, 1210, 3355, 72441, 715, 6, 33939, 1210, 8389, 532, 2, 15, 21, 4615, 2550, 1969, 1795, 419, 3561, 3897, 624, 515, 262, 330, 60565, 82, 788, 330, 7771, 11303, 389, 279, 3119, 304, 5810, 4128, 756, 262, 330, 53103, 13009, 788, 4383, 8092, 307, 3252, 915, 2198, 19895, 3252, 785, 7966, 369, 6529, 7026, 262, 330, 1311, 2019, 788, 2278, 286, 5212, 606, 788, 330, 1311, 829, 497, 330, 17119, 367, 788, 330, 12332, 279, 3601, 3059, 315, 419, 1917, 7115, 286, 5212, 606, 788, 330, 1311, 829, 497, 330, 17119, 367, 788, 330, 12332, 279, 3601, 3059, 315, 419, 1917, 7115, 286, 5212, 606, 788, 330, 1311, 829, 497, 330, 17119, 367, 788, 330, 12332, 279, 3601, 3059, 315, 419, 1917, 16707, 262, 5133, 532, 785, 1973, 304, 892, 6168, 525, 15695, 646, 387, 5189, 304, 330, 1311, 2019, 22956, 2, 15, 22, 1817, 1917, 1969, 387, 304, 279, 2701, 738, 11, 323, 498, 525, 537, 5420, 311, 1855, 501, 6174, 624, 9640, 16, 79520, 25515, 287, 2895, 7386, 320, 1986, 374, 458, 1917, 10660, 553, 1638, 11, 323, 979, 1052, 374, 902, 9842, 15048, 22703, 3281, 6380, 11, 279, 38193, 7310, 686, 728, 7678, 8305, 518, 264, 6783, 4628, 4092, 311, 279, 12909, 1815, 13, 1326, 17, 97075, 25515, 2895, 7386, 345, 18, 45634, 9579, 345, 19, 45634, 10539, 345, 20, 77992, 58668, 345, 21, 2285, 3672, 58668, 345, 22, 918, 2145, 15947, 9579, 345, 23, 918, 2145, 15947, 10539, 320, 2610, 1265, 2908, 10018, 33025, 979, 498, 1969, 2297, 697, 21272, 304, 1973, 311, 387, 2952, 311, 5545, 279, 1790, 6821, 389, 697, 6021, 26, 311, 1492, 2441, 7310, 2297, 311, 279, 21272, 807, 525, 304, 26, 311, 5648, 2701, 264, 6301, 7218, 7310, 26, 323, 311, 2506, 311, 279, 1290, 438, 1753, 438, 3204, 979, 24722, 525, 3347, 1326, 24, 40506, 588, 9579, 320, 4498, 264, 3347, 29599, 476, 52635, 32187, 374, 389, 279, 1290, 3108, 315, 697, 9115, 1815, 11, 498, 1265, 5648, 432, 311, 279, 2115, 1326, 16, 15, 40506, 588, 10539, 320, 4498, 264, 3347, 29599, 476, 52635, 32187, 374, 389, 279, 2115, 3108, 315, 697, 9115, 1815, 11, 498, 1265, 5648, 432, 311, 279, 1290, 1326, 16, 16, 4962, 320, 1986, 1917, 3363, 279, 1882, 553, 892, 264, 7310, 14177, 973, 504, 264, 2936, 892, 374, 537, 279, 1852, 438, 151645, 198, 151644, 77091, 198, 515, 262, 330, 60565, 82, 788, 330, 28715, 389, 279, 1482, 9442, 5601, 323, 279, 36274, 7310, 594, 2639, 11, 432, 4977, 1075, 279, 1850, 3308, 315, 1917, 374, 311, 2506, 2087, 7678, 518, 264, 6783, 4628, 10346, 262, 330, 53103, 13009, 788, 10239, 262, 330, 1311, 2019, 788, 2278, 286, 5212, 606, 788, 330, 13096, 25515, 287, 2895, 7386, 497, 330, 17119, 367, 788, 330, 785, 38193, 7310, 686, 3060, 2087, 7678, 518, 264, 6783, 4628, 4092, 311, 279, 12909, 1815, 92181, 286, 5212, 606, 788, 330, 73648, 25515, 2895, 7386, 497, 330, 17119, 367, 788, 330, 2679, 279, 5795, 374, 311, 728, 7678, 518, 458, 19246, 11, 279, 7310, 686, 10354, 1526, 279, 19246, 92181, 286, 5212, 606, 788, 330, 412, 9579, 497, 330, 17119, 367, 788, 330, 2679, 279, 5795, 374, 311, 2484, 2115, 11, 279, 7310, 686, 6301, 1495, 323, 1281, 264, 2115, 2484, 518, 279, 8311, 882, 92181, 286, 5212, 606, 788, 330, 412, 10539, 497, 330, 17119, 367, 788, 330, 2679, 279, 5795, 374, 311, 2484, 1290, 11, 279, 7310, 686, 6301, 1495, 323, 1281, 264, 1290, 2484, 518, 279, 8311, 882, 92181, 286, 5212, 606, 788, 330, 43888, 58668, 497, 330, 17119, 367, 788, 330, 2679, 279, 5795, 374, 311, 42780, 11, 279, 7310, 686, 5263, 1181, 4628, 27079, 92181, 286, 5212, 606, 788, 330, 450, 3672, 58668, 497, 330, 17119, 367, 788, 330, 2679, 279, 5795, 374, 311, 1622, 7865, 349, 11, 279, 7310, 686, 6301, 1495, 27079, 92181, 286, 5212, 606, 788, 330, 37847, 15947, 9579, 497, 330, 17119, 367, 788, 330, 2679, 5871, 11, 279, 7310, 686, 2297, 33025, 311, 279, 2115, 304, 1973, 311, 5545, 279, 1790, 6821, 389, 279, 6021, 11, 1492, 2441, 7310, 2297, 33025, 11, 5648, 264, 6301, 7218, 7310, 11, 476, 2506, 311, 279, 1290, 518, 3347, 24722, 92181, 286, 5212, 606, 788, 330, 37847, 15947, 10539, 497, 330, 17119, 367, 788, 330, 2679, 5871, 11, 279, 7310, 686, 2297, 33025, 311, 279, 1290, 304, 1973, 311, 5545, 279, 1790, 6821, 389, 279, 6021, 11, 1492, 2441, 7310, 2297, 33025, 11, 5648, 264, 6301, 7218, 7310, 11, 476, 2506, 311, 279, 1290, 518, 3347, 24722, 92181, 286, 5212, 606, 788, 330, 47856, 9579, 497, 330, 17119, 367, 788, 330, 2679, 1052, 374, 264, 3347, 29599, 476, 52635, 32187, 389, 279, 1290, 3108, 315, 279, 12909, 1815, 11, 279, 7310, 686, 5648, 432, 553, 7218, 311, 279, 2115, 92181, 286, 5212, 606, 788, 330, 47856, 10539, 497, 330, 17119, 367, 788, 330, 2679, 1052, 374, 264, 3347, 29599, 476, 52635, 32187, 389, 279, 2115, 3108, 315, 279, 12909, 1815, 11, 279, 7310, 686, 5648, 432, 553, 7218, 311, 279, 1290, 92181, 286, 5212, 606, 788, 330, 2468, 497, 330, 17119, 367, 788, 330, 785, 7310, 686, 3161, 68641, 504, 264, 2936, 11, 421, 5871, 1189, 532, 262, 5133, 92, 151645, 198]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84fb5f460004278b842cdb638622376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/26 shards):   0%|          | 0/7667416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(tokenized_dataset[0:5])\n",
    "tokenized_dataset.save_to_disk(\"./tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "340f3fd7-a707-4596-9c42-c42b8ec295ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\n0.002 = 1000 \\n1 = x?<|im_end|>\\n<|im_start|>assistant\\nTo find the value of x, we can set up a proportion using the given information:\\n\\n0.002/1000 = 1/x\\n\\nTo solve for x, we can cross multiply:\\n\\n0.002 * x = 1000 * 1\\n\\n0.002x = 1000\\n\\nDividing both sides by 0.002:\\n\\nx = 1000 / 0.002\\n\\nx = 500,000\\n\\nTherefore, 1 is equal to 500,000 in this proportion.<|im_end|>\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.decode(tokenized_dataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1c089f2-f131-483c-8444-13e8e4c8357b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '0.002 = 1000 \\n1 = x?', 'completion': 'To find the value of x, we can set up a proportion using the given information:\\n\\n0.002/1000 = 1/x\\n\\nTo solve for x, we can cross multiply:\\n\\n0.002 * x = 1000 * 1\\n\\n0.002x = 1000\\n\\nDividing both sides by 0.002:\\n\\nx = 1000 / 0.002\\n\\nx = 500,000\\n\\nTherefore, 1 is equal to 500,000 in this proportion.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7853727-3f90-4d89-b443-bcc844e5f0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98f563dcfd94dea92e6d78611e16d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/7667416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m logging.getLogger(\u001b[33m\"\u001b[39m\u001b[33mtrl.trainer.sft_trainer\u001b[39m\u001b[33m\"\u001b[39m).setLevel(logging.ERROR)\n\u001b[32m      4\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen1.5-MoE-A2.7B\u001b[39m\u001b[33m\"\u001b[39m, use_fast=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mSFTConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup_by_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madamw_8bit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcosine\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3407\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./outputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m torch.cuda.empty_cache()\n\u001b[32m     33\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:805\u001b[39m, in \u001b[36mSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[39m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.completion_only_loss \u001b[38;5;129;01mand\u001b[39;00m formatting_func:\n\u001b[32m    799\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    800\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA formatting function was provided while `completion_only_loss=True`, which is incompatible. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    801\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing a formatter converts the dataset to a language modeling type, conflicting with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    802\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcompletion-only loss. To resolve this, apply your formatting function before passing the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    803\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdataset, or disable `completion_only_loss` in `SFTConfig`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    804\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m train_dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    807\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    809\u001b[39m     packing = args.packing \u001b[38;5;28;01mif\u001b[39;00m args.eval_packing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args.eval_packing\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:1035\u001b[39m, in \u001b[36mSFTTrainer._prepare_dataset\u001b[39m\u001b[34m(self, dataset, processing_class, args, packing, formatting_func, dataset_name)\u001b[39m\n\u001b[32m   1027\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1028\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mYou\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre using `assistant_only_loss=True`, but at least one example has no assistant \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtokens. This usually means the tokenizer\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms chat template doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt generate assistant \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1030\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmasks â€” it may be missing the `\u001b[39m\u001b[33m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[33meneration \u001b[39m\u001b[33m%\u001b[39m\u001b[33m}` keyword. Please check the template and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1031\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mensure it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms correctly configured to support assistant masking.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1032\u001b[39m             )\n\u001b[32m   1033\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m     dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenize_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprocessing_class\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset_text_field\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43massistant_only_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43massistant_only_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmap_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[38;5;66;03m# Pack or truncate\u001b[39;00m\n\u001b[32m   1046\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m packing:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3341\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3339\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3340\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3341\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3342\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3673\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3671\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[32m   3672\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3673\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3674\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3675\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3647\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3645\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3646\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3647\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3570\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3568\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3569\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3570\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3571\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:988\u001b[39m, in \u001b[36mSFTTrainer._prepare_dataset.<locals>.tokenize_fn\u001b[39m\u001b[34m(example, processing_class, dataset_text_field, assistant_only_loss)\u001b[39m\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    987\u001b[39m     prompt_ids = processing_class(text=example[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m])[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m     prompt_completion_ids = \u001b[43mprocessing_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompletion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    989\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    990\u001b[39m     ]\n\u001b[32m    992\u001b[39m \u001b[38;5;66;03m# Check if the tokenized prompt starts with the tokenized prompt+completion\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prompt_completion_ids[: \u001b[38;5;28mlen\u001b[39m(prompt_ids)] == prompt_ids:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2938\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2936\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2937\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2938\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2939\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2940\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3048\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3026\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   3027\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   3028\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3045\u001b[39m         **kwargs,\n\u001b[32m   3046\u001b[39m     )\n\u001b[32m   3047\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3048\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3051\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3063\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3064\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3065\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3066\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3067\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3068\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3123\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3094\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3095\u001b[39m \u001b[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[32m   3096\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3111\u001b[39m \u001b[33;03m        method).\u001b[39;00m\n\u001b[32m   3112\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3114\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3115\u001b[39m     padding=padding,\n\u001b[32m   3116\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3120\u001b[39m     **kwargs,\n\u001b[32m   3121\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3142\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:627\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode_plus\u001b[39m(\n\u001b[32m    604\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    605\u001b[39m     text: Union[TextInput, PreTokenizedInput],\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m     **kwargs,\n\u001b[32m    625\u001b[39m ) -> BatchEncoding:\n\u001b[32m    626\u001b[39m     batched_input = [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     batched_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[32m    650\u001b[39m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[32m    651\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:553\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001b[32m    551\u001b[39m     \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;66;03m# `Tokens` has type: tuple[\u001b[39;00m\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m#                       list[dict[str, list[list[int]]]] or list[dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m#                       list[EncodingFast]\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[32m    565\u001b[39m tokens_and_encodings = [\n\u001b[32m    566\u001b[39m     \u001b[38;5;28mself\u001b[39m._convert_encoding(\n\u001b[32m    567\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[32m    577\u001b[39m ]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig\n",
    "import logging\n",
    "logging.getLogger(\"trl.trainer.sft_trainer\").setLevel(logging.ERROR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B\", use_fast=True)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = lora_model,\n",
    "    train_dataset = dataset,\n",
    "    processing_class = tokenizer,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 16,\n",
    "        packing = True,\n",
    "        group_by_length = True,\n",
    "        warmup_steps = 5,\n",
    "        bf16 = True,\n",
    "        max_steps=10000,\n",
    "        learning_rate = 2e-4,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        eval_strategy=\"no\",\n",
    "        do_eval=False,\n",
    "        output_dir = \"./outputs\",\n",
    "        push_to_hub=True,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(\"Qwen1.5_MoE_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94fc05b-37d5-4813-a0f1-c1b668bbd0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
