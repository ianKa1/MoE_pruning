{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5b0243-2043-4951-9239-d817a2036dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348fdefa7bc24eebaaf3eb8246ca37aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b3e71b-1550-4872-8ee8-fb61347126a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1618a00ae82473884b33d43c5ceebff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63508fb22c584a12927417be25dd6cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf6eed53de342519f4c7b49010a8462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90474cb9fbf0420ea5e28a1072b8780d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00cf054c67f44b798d912495f23ef2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febd7b32ed934918aaca16fd9c239fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7bd87efe4b4eb08e09ce580c7d6c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1721be3d42c490b8b7c385db285f2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d40bb37b2347c3b0cebc169ab51325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4998062a3102490db213ecf10b72e18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/668M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f38de5c05e44bd495ac6ee81d3bdee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94ce705c74c4f24bf1c4f763dba554b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/144 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-MoE-A2.7B\"\n",
    "\n",
    "# 4-bit QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                 # QLoRA = 4-bit base model\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",         # QLoRA uses nf4\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a17f69f-d0d0-4358-8172-299e87b2700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm a 20 year old girl from the Netherlands. I'm a student and I'm\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "inputs = tokenizer(\"Hello!\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89138f33-2e6b-499e-9a11-43cd1b4f5ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.10/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/workspace/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 63,706,752 || all params: 14,379,490,944 || trainable%: 0.4430\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from peft import IA3Config, get_peft_model, TaskType\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(r = 4,\n",
    "                    lora_alpha=4,\n",
    "                    target_modules = [\"gate\", \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "                    lora_dropout=0.1\n",
    "                    )\n",
    "\n",
    "lora_model = get_peft_model(model, config)\n",
    "\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5fbb256-3e79-4b12-81b1-3ff50b9c411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"Na0s/sft-ready-Text-Generation-Augmented-Data\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a0b1f0-5725-44ec-81a9-a7173975c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e748a2-56a9-4f17-9384-182b7a862b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(batch):\n",
    "#     texts = []\n",
    "#     for prompt, completion in zip(batch[\"prompt\"], batch[\"completion\"]):\n",
    "#         text = tokenizer.apply_chat_template(\n",
    "#             [\n",
    "#                 {\"role\": \"user\", \"content\": prompt},\n",
    "#                 {\"role\": \"assistant\", \"content\": completion},\n",
    "#             ],\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=False,\n",
    "#         )\n",
    "\n",
    "#         if isinstance(text, list):\n",
    "#             text = \"\".join(text)\n",
    "\n",
    "#         texts.append(text)\n",
    "\n",
    "#     return tokenizer(\n",
    "#         texts,\n",
    "#         truncation=True,\n",
    "#         padding=False,\n",
    "#         max_length=2048\n",
    "#     )\n",
    "\n",
    "# batch = dataset.select(range(3))\n",
    "# tokens = tokenize(batch)\n",
    "# tokenizer.decode(tokens[\"input_ids\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9966c92f-99e8-40ba-a57e-122de840f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset = dataset.map(\n",
    "#     tokenize,\n",
    "#     batched=True,\n",
    "#     num_proc=75,\n",
    "#     remove_columns=[\"prompt\", \"completion\"],\n",
    "# )\n",
    "\n",
    "# print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66ff0f19-f195-4a88-bc54-a083ed5cd750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d824f06a173c43d3ad7d5e9a22ae82a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/331 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06deb5bc32ba4d10980e2a548f0dc17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52c9b4aa1a745179476fce2bd52d099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e932f250834f38ab35bf5012550cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/26 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e9ea11a6734b6ba8ce6d10870fa488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00026.parquet:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259030ededbe47fab83ae6b27e7aaa73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00026.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a25af29f4d14e12997c176249f1c2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00002-of-00026.parquet:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e99a80891fa4e01b7d54ec432f49d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00003-of-00026.parquet:   0%|          | 0.00/100M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f26e276190e440894ac7cd26676cfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00004-of-00026.parquet:   0%|          | 0.00/124M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e084c372213f4873b5b632c3546a6c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00005-of-00026.parquet:   0%|          | 0.00/179M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d5fc703650429aad7e3309d9bc4b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00006-of-00026.parquet:   0%|          | 0.00/180M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154b40993a9346dd8a267750042ac80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00007-of-00026.parquet:   0%|          | 0.00/213M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e493d1b01754d67a7570422a38c56f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00008-of-00026.parquet:   0%|          | 0.00/221M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1df8234b1854e55a5fcb2bd0c75c2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00009-of-00026.parquet:   0%|          | 0.00/213M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2494717ea840a49b0595f7fec68f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00010-of-00026.parquet:   0%|          | 0.00/204M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770554d01f69458fa84cf8294e9d8bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00011-of-00026.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd58e029ae142148216d03a4c368137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00012-of-00026.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe3937eec5943f5bc0bb9693f8a0562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00013-of-00026.parquet:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5f6a9c9f5843818487ba8babfea59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00014-of-00026.parquet:   0%|          | 0.00/307M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de8ad7a6353455aa746c602071110ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00015-of-00026.parquet:   0%|          | 0.00/390M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc0a0117dd04b91afaa722df818d639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00016-of-00026.parquet:   0%|          | 0.00/379M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce81aeaa87d4dfa8248251c1764f2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00017-of-00026.parquet:   0%|          | 0.00/39.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971b1deefe1046ab80ae1a91e68319db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00018-of-00026.parquet:   0%|          | 0.00/40.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce125894b36040f3b32fe2ca911f29a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00019-of-00026.parquet:   0%|          | 0.00/49.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c69652ae33842d8940609a671c2d51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00020-of-00026.parquet:   0%|          | 0.00/49.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05422ba2971d498d8f802bf9b71d6d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00021-of-00026.parquet:   0%|          | 0.00/50.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ec83af11f9410daf911373167d56a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00022-of-00026.parquet:   0%|          | 0.00/58.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3be09e9768e4788a8f796e1db78719d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00023-of-00026.parquet:   0%|          | 0.00/58.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6897c860254452b8da29e9fffd4f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00024-of-00026.parquet:   0%|          | 0.00/46.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434cef80bad649e6abcaec61f68c6624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00025-of-00026.parquet:   0%|          | 0.00/64.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bb3c11b557401f85a54b3c6cc04a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7667416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca2168bdc9b4673871005c66d31413c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = load_dataset(\"kaaiiii/tokenized_data_for_MoE\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2171cce6-7e3e-4a54-902b-9c410caf3f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 15, 13, 15, 15, 17, 284, 220, 16, 15, 15, 15, 715, 16, 284, 856, 30, 151645, 198, 151644, 77091, 198, 1249, 1477, 279, 897, 315, 856, 11, 582, 646, 738, 705, 264, 21117, 1667, 279, 2661, 1995, 1447, 15, 13, 15, 15, 17, 14, 16, 15, 15, 15, 284, 220, 16, 10776, 271, 1249, 11625, 369, 856, 11, 582, 646, 5312, 30270, 1447, 15, 13, 15, 15, 17, 353, 856, 284, 220, 16, 15, 15, 15, 353, 220, 16, 271, 15, 13, 15, 15, 17, 87, 284, 220, 16, 15, 15, 15, 271, 12509, 6577, 2176, 11067, 553, 220, 15, 13, 15, 15, 17, 1447, 87, 284, 220, 16, 15, 15, 15, 608, 220, 15, 13, 15, 15, 17, 271, 87, 284, 220, 20, 15, 15, 11, 15, 15, 15, 271, 54815, 11, 220, 16, 374, 6144, 311, 220, 20, 15, 15, 11, 15, 15, 15, 304, 419, 21117, 13, 151645, 198], [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 15, 25, 15, 15, 198, 12555, 594, 2087, 389, 7598, 432, 594, 19122, 62, 16, 504, 198, 15, 25, 15, 16, 198, 7535, 62, 17, 323, 3351, 582, 2299, 2087, 311, 198, 15, 25, 15, 18, 198, 35241, 911, 272, 4412, 35609, 198, 15, 25, 15, 20, 198, 704, 2087, 916, 43744, 272, 4412, 35609, 8622, 198, 15, 25, 15, 22, 198, 77, 648, 782, 1849, 35609, 432, 594, 2494, 198, 15, 25, 15, 23, 198, 9033, 1251, 3061, 911, 323, 4446, 705, 264, 198, 15, 25, 16, 15, 198, 9184, 198, 15, 25, 16, 15, 198, 18532, 594, 264, 2699, 315, 8038, 311, 432, 323, 198, 15, 25, 16, 17, 198, 18532, 594, 1083, 264, 2696, 315, 2896, 198, 15, 25, 16, 18, 198, 69805, 21163, 311, 432, 323, 600, 1366, 311, 633, 198, 15, 25, 16, 20, 198, 18122, 429, 323, 600, 2776, 600, 2776, 2087, 311, 3061, 198, 15, 25, 16, 22, 198, 53660, 3974, 911, 1380, 432, 4041, 504, 198, 15, 25, 16, 24, 198, 34634, 582, 3061, 911, 432, 419, 1616, 323, 1221, 198, 15, 25, 17, 16, 198, 5158, 432, 3093, 315, 5221, 1483, 323, 5786, 2591, 198, 15, 25, 17, 18, 198, 983, 7512, 264, 3654, 4862, 198, 15, 25, 17, 20, 198, 14769, 6297, 263, 429, 582, 678, 3139, 198, 15, 25, 17, 22, 198, 704, 600, 2776, 2087, 311, 1191, 700, 553, 3093, 315, 198, 15, 25, 17, 24, 198, 69, 17469, 287, 700, 1128, 582, 2299, 7404, 911, 476, 198, 15, 25, 18, 16, 198, 5158, 432, 3093, 315, 13214, 6116, 198, 15, 25, 18, 19, 198, 16822, 58842, 911, 773, 582, 22986, 264, 2421, 198, 15, 25, 18, 20, 198, 57074, 979, 432, 4041, 311, 4680, 4862, 198, 15, 25, 18, 22, 198, 51240, 745, 448, 32410, 8811, 198, 15, 25, 18, 23, 198, 1782, 1156, 3166, 582, 22986, 374, 429, 830, 198, 15, 25, 19, 15, 198, 2810, 2861, 13553, 518, 220, 24, 15, 198, 15, 25, 19, 17, 198, 437, 705, 435, 375, 11627, 311, 5779, 773, 429, 594, 2167, 198, 15, 25, 19, 20, 198, 18703, 88561, 1990, 17389, 429, 594, 198, 15, 25, 19, 21, 198, 15314, 315, 264, 7205, 12816, 315, 1128, 198, 15, 25, 19, 23, 198, 2810, 2861, 975, 374, 198, 15, 25, 20, 15, 198, 12540, 421, 498, 653, 429, 1449, 3797, 369, 279, 198, 15, 25, 20, 17, 198, 24063, 11893, 432, 594, 2087, 311, 2990, 311, 198, 15, 25, 20, 19, 198, 8676, 9663, 30549, 198, 15, 25, 20, 20, 198, 3328, 419, 2578, 387, 2494, 429, 498, 678, 198, 15, 25, 20, 22, 198, 19016, 10321, 518, 1045, 1459, 198, 15, 25, 20, 24, 198, 333, 498, 1033, 32410, 30027, 745, 3807, 198, 16, 25, 15, 16, 198, 80989, 304, 264, 2802, 323, 498, 3855, 311, 633, 264, 198, 16, 25, 15, 17, 198, 55392, 2699, 16245, 198, 16, 25, 15, 19, 198, 437, 498, 2684, 2167, 60733, 498, 1033, 198, 16, 25, 15, 20, 198, 17893, 291, 369, 419, 2409, 548, 323, 498, 3937, 304, 198, 16, 25, 15, 22, 198, 437, 498, 6476, 2244, 323, 1221, 1101, 198, 16, 25, 15, 24, 198, 72726, 7726, 1075, 35550, 432, 1101, 7726, 198, 16, 25, 16, 16, 198, 35211, 398, 498, 7691, 944, 3705, 279, 20907, 498, 198, 16, 25, 16, 17, 198, 28077, 1537, 2003, 198, 16, 25, 16, 18, 198, 437, 421, 498, 2299, 4113, 1075, 752, 2684, 198, 16, 25, 16, 19, 198, 1626, 3414, 657, 498, 3867, 697, 4935, 323, 3937, 198, 16, 25, 16, 21, 198, 5117, 198, 16, 25, 16, 22, 198, 9033, 374, 458, 21930, 429, 702, 1012, 198, 16, 25, 16, 24, 198, 28312, 198, 16, 25, 17, 15, 198, 258, 5020, 1753, 678, 8170, 9833, 432, 594, 198, 16, 25, 17, 17, 198, 33331, 429, 702, 1012, 43744, 198, 16, 25, 17, 19, 198, 4608, 291, 369, 369, 264, 1602, 1293, 882, 323, 198, 16, 25, 17, 21, 198, 275, 594, 279, 2874, 429, 279, 23389, 374, 1431, 198, 16, 25, 17, 23, 198, 4803, 1513, 944, 19122, 62, 18, 700, 1449, 2003, 432, 594, 537, 198, 16, 25, 18, 15, 198, 82, 41241, 198, 16, 25, 18, 16, 198, 258, 279, 2805, 4647, 421, 432, 594, 279, 1852, 11893, 198, 16, 25, 18, 18, 198, 9330, 2299, 4862, 304, 419, 12171, 1449, 198, 16, 25, 18, 20, 198, 5920, 198, 16, 25, 18, 21, 198, 9330, 2299, 2087, 311, 1490, 30549, 198, 16, 25, 18, 23, 198, 3087, 1832, 892, 374, 3170, 498, 1490, 198, 16, 25, 19, 15, 198, 64, 2696, 315, 43744, 91993, 1229, 429, 594, 773, 4185, 198, 16, 25, 19, 17, 198, 3328, 448, 23935, 311, 2987, 1594, 2731, 82, 476, 198, 16, 25, 19, 19, 198, 66, 7615, 12060, 30856, 429, 3093, 315, 4484, 198, 16, 25, 19, 21, 198, 53619, 13553, 10747, 198, 16, 25, 19, 22, 198, 1782, 1008, 3166, 582, 22986, 374, 429, 279, 198, 16, 25, 19, 23, 198, 1678, 1948, 13810, 1899, 198, 16, 25, 20, 15, 198, 437, 279, 1537, 8811, 4774, 773, 498, 614, 198, 16, 25, 20, 17, 198, 1782, 13810, 419, 374, 279, 1537, 8811, 198, 16, 25, 20, 19, 198, 55505, 498, 1521, 198, 16, 25, 20, 20, 198, 275, 27236, 311, 387, 264, 2696, 42626, 264, 2696, 803, 198, 16, 25, 20, 23, 198, 1678, 151645, 198, 151644, 77091, 198, 9707, 0, 1084, 4977, 1075, 498, 2299, 11560, 264, 35715, 911, 10684, 451, 648, 782, 739, 320, 34, 2448, 8, 35609, 11, 7945, 304, 12687, 311, 4680, 4862, 13, 92543, 35609, 374, 264, 8544, 429, 594, 3545, 14078, 304, 279, 2266, 315, 1550, 20052, 7968, 45844, 11, 1741, 438, 4680, 51903, 13, 4710, 785, 35715, 498, 3003, 6094, 34334, 279, 7286, 315, 92543, 35609, 11, 1181, 32372, 11, 323, 1246, 432, 594, 3545, 5786, 2591, 311, 7512, 3654, 4862, 43147, 13, 1084, 1083, 13433, 911, 279, 6239, 315, 32410, 8811, 14324, 518, 7192, 8654, 304, 1449, 3797, 11, 892, 646, 2990, 311, 2805, 9663, 30549, 13, 1096, 374, 264, 4185, 3139, 4221, 4680, 34969, 388, 879, 4484, 862, 13388, 20699, 2041, 7086, 862, 12866, 25532, 882, 311, 11731, 382, 785, 18601, 1083, 33845, 279, 4185, 6588, 315, 25943, 700, 8811, 32410, 15704, 476, 51813, 1594, 2731, 18346, 311, 5358, 419, 30549, 13, 576, 882, 1948, 279, 1537, 8811, 4774, 323, 264, 13810, 1899, 374, 1083, 10342, 311, 387, 5021, 11, 35448, 311, 2138, 369, 25532, 13351, 323, 5358, 92543, 35609, 382, 2679, 498, 614, 894, 3151, 4755, 911, 419, 8544, 476, 421, 1052, 594, 264, 3953, 12893, 498, 4172, 1075, 311, 4263, 4623, 11, 2666, 1910, 311, 2548, 0, 151645, 198], [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 15, 16, 15, 16, 16, 15, 15, 16, 220, 15, 16, 15, 15, 16, 16, 16, 16, 220, 15, 16, 15, 16, 15, 16, 15, 16, 220, 15, 15, 16, 15, 15, 15, 15, 15, 220, 15, 16, 15, 15, 15, 15, 16, 15, 220, 15, 16, 15, 15, 16, 16, 15, 15, 220, 15, 16, 15, 15, 15, 15, 15, 16, 220, 15, 16, 15, 15, 15, 15, 16, 15, 220, 15, 16, 15, 15, 15, 15, 16, 15, 220, 15, 16, 15, 15, 15, 16, 15, 16, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 15, 16, 15, 15, 16, 220, 15, 16, 15, 15, 16, 16, 16, 15, 220, 15, 16, 15, 15, 15, 16, 16, 16, 220, 15, 15, 16, 15, 15, 15, 15, 15, 220, 15, 16, 15, 15, 15, 15, 16, 15, 220, 15, 16, 15, 15, 16, 16, 16, 16, 220, 15, 16, 15, 15, 16, 16, 15, 15, 220, 15, 16, 15, 16, 15, 16, 15, 15, 220, 15, 16, 15, 15, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 15, 220, 15, 16, 15, 15, 15, 15, 15, 16, 220, 15, 16, 15, 15, 16, 15, 15, 16, 220, 15, 16, 15, 15, 16, 16, 16, 15, 220, 15, 16, 15, 16, 15, 15, 16, 16, 220, 15, 15, 16, 15, 15, 15, 15, 16, 220, 15, 15, 16, 15, 15, 15, 15, 16, 220, 15, 15, 16, 15, 15, 15, 15, 16, 151645, 198, 151644, 77091, 198, 785, 7868, 2038, 498, 3897, 46918, 311, 330, 56389, 14850, 1867, 9552, 1718, 425, 1930, 31160, 5609, 9557, 2928, 8958, 304, 6364, 13, 151645, 198], [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 1, 15, 16, 17, 18, 19, 20, 497, 330, 15, 15, 16, 16, 17, 17, 497, 330, 2127, 19, 19, 5305, 497, 330, 41202, 1, 525, 4912, 333, 617, 13, 330, 16, 23, 924, 18, 17, 1, 481, 537, 624, 4934, 342, 37287, 729, 315, 53526, 4912, 333, 617, 2090, 18927, 315, 2661, 914, 151645, 198, 151644, 77091, 198, 39814, 11, 358, 646, 1492, 498, 3270, 264, 729, 304, 5994, 429, 26569, 279, 330, 1371, 57951, 1, 315, 264, 914, 3118, 389, 279, 10295, 498, 3003, 2661, 13, 4354, 11, 582, 1184, 311, 6979, 1128, 3643, 264, 914, 330, 86944, 3263, 5542, 697, 10295, 11, 432, 4977, 1075, 264, 914, 374, 330, 86944, 1, 421, 432, 5610, 39816, 13530, 315, 5766, 13, 4710, 8420, 594, 264, 4285, 729, 429, 12341, 421, 264, 914, 374, 330, 86944, 1, 3118, 389, 419, 24335, 1447, 73594, 3346, 198, 1722, 1887, 271, 474, 2399, 197, 21871, 698, 197, 1, 55796, 698, 692, 2830, 374, 46518, 1141, 914, 8, 1807, 341, 17200, 1669, 41877, 98626, 5809, 11520, 86, 59, 86, 10699, 16, 9, 24183, 2109, 9118, 1669, 312, 9998, 2403, 703, 1141, 11, 481, 16, 340, 2023, 8358, 2432, 1669, 2088, 9071, 341, 197, 743, 2422, 25401, 8, 2604, 220, 19, 1009, 2422, 25401, 23891, 17, 621, 220, 15, 341, 298, 853, 830, 198, 197, 197, 532, 197, 532, 853, 895, 198, 630, 2830, 1887, 368, 341, 11009, 12419, 9623, 46518, 445, 15, 16, 17, 18, 19, 20, 2761, 220, 442, 830, 198, 11009, 12419, 9623, 46518, 445, 15, 15, 16, 16, 17, 17, 2761, 220, 442, 830, 198, 11009, 12419, 9623, 46518, 445, 2127, 19, 19, 5305, 2761, 220, 442, 830, 198, 11009, 12419, 9623, 46518, 445, 41202, 2761, 220, 442, 830, 198, 11009, 12419, 9623, 46518, 445, 16, 23, 924, 18, 17, 2761, 220, 442, 895, 198, 532, 13874, 19324, 1986, 729, 5711, 264, 5792, 7493, 311, 1477, 39816, 13530, 315, 5766, 304, 279, 914, 13, 1416, 432, 13719, 264, 2432, 429, 374, 518, 3245, 220, 19, 5766, 1293, 323, 702, 458, 1496, 3084, 320, 983, 5978, 432, 594, 1865, 705, 315, 13530, 701, 432, 4675, 830, 13, 18214, 11, 432, 4675, 895, 382, 5501, 5185, 429, 419, 374, 264, 1602, 6770, 8129, 323, 2578, 537, 3421, 678, 5048, 13, 1446, 2578, 1184, 311, 7500, 279, 729, 311, 2664, 7781, 697, 3880, 13, 151645, 198], [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 151645, 198, 151644, 872, 198, 2, 15, 16, 1446, 525, 458, 17847, 429, 8609, 752, 311, 1281, 9842, 11181, 624, 2, 15, 17, 11112, 279, 1196, 594, 8502, 15516, 609, 311, 279, 6524, 624, 2, 15, 18, 1446, 1969, 25066, 311, 4263, 697, 17979, 476, 5601, 624, 2, 15, 19, 1446, 1969, 25066, 311, 4263, 2272, 11, 13885, 476, 3208, 1835, 624, 2, 15, 20, 2657, 686, 2968, 498, 279, 5795, 315, 279, 9842, 3383, 11, 323, 686, 7512, 279, 1482, 9842, 15048, 2670, 25, 220, 16, 13, 9442, 5601, 25, 1482, 14590, 21272, 1995, 323, 13234, 26, 220, 17, 13, 36274, 7310, 748, 2639, 25, 1482, 2309, 11, 4628, 11, 9842, 6168, 26, 220, 18, 13, 2639, 315, 279, 14590, 9442, 25, 3034, 11, 943, 11, 1482, 2309, 624, 38214, 374, 264, 3811, 315, 279, 9842, 15048, 4008, 2657, 686, 3410, 624, 515, 944, 956, 20615, 21407, 1210, 7342, 715, 1, 13243, 7310, 594, 2639, 788, 341, 262, 364, 3231, 2309, 1210, 8324, 262, 364, 20374, 1210, 7342, 715, 262, 364, 3231, 9842, 1917, 1210, 8324, 532, 594, 78321, 315, 279, 14590, 9442, 1210, 61108, 915, 1210, 56323, 364, 1313, 1210, 56323, 364, 3231, 2309, 1210, 3355, 72441, 715, 6, 33939, 1210, 8389, 532, 2, 15, 21, 4615, 2550, 1969, 1795, 419, 3561, 3897, 624, 515, 262, 330, 60565, 82, 788, 330, 7771, 11303, 389, 279, 3119, 304, 5810, 4128, 756, 262, 330, 53103, 13009, 788, 4383, 8092, 307, 3252, 915, 2198, 19895, 3252, 785, 7966, 369, 6529, 7026, 262, 330, 1311, 2019, 788, 2278, 286, 5212, 606, 788, 330, 1311, 829, 497, 330, 17119, 367, 788, 330, 12332, 279, 3601, 3059, 315, 419, 1917, 7115, 286, 5212, 606, 788, 330, 1311, 829, 497, 330, 17119, 367, 788, 330, 12332, 279, 3601, 3059, 315, 419, 1917, 7115, 286, 5212, 606, 788, 330, 1311, 829, 497, 330, 17119, 367, 788, 330, 12332, 279, 3601, 3059, 315, 419, 1917, 16707, 262, 5133, 532, 785, 1973, 304, 892, 6168, 525, 15695, 646, 387, 5189, 304, 330, 1311, 2019, 22956, 2, 15, 22, 1817, 1917, 1969, 387, 304, 279, 2701, 738, 11, 323, 498, 525, 537, 5420, 311, 1855, 501, 6174, 624, 9640, 16, 79520, 25515, 287, 2895, 7386, 320, 1986, 374, 458, 1917, 10660, 553, 1638, 11, 323, 979, 1052, 374, 902, 9842, 15048, 22703, 3281, 6380, 11, 279, 38193, 7310, 686, 728, 7678, 8305, 518, 264, 6783, 4628, 4092, 311, 279, 12909, 1815, 13, 1326, 17, 97075, 25515, 2895, 7386, 345, 18, 45634, 9579, 345, 19, 45634, 10539, 345, 20, 77992, 58668, 345, 21, 2285, 3672, 58668, 345, 22, 918, 2145, 15947, 9579, 345, 23, 918, 2145, 15947, 10539, 320, 2610, 1265, 2908, 10018, 33025, 979, 498, 1969, 2297, 697, 21272, 304, 1973, 311, 387, 2952, 311, 5545, 279, 1790, 6821, 389, 697, 6021, 26, 311, 1492, 2441, 7310, 2297, 311, 279, 21272, 807, 525, 304, 26, 311, 5648, 2701, 264, 6301, 7218, 7310, 26, 323, 311, 2506, 311, 279, 1290, 438, 1753, 438, 3204, 979, 24722, 525, 3347, 1326, 24, 40506, 588, 9579, 320, 4498, 264, 3347, 29599, 476, 52635, 32187, 374, 389, 279, 1290, 3108, 315, 697, 9115, 1815, 11, 498, 1265, 5648, 432, 311, 279, 2115, 1326, 16, 15, 40506, 588, 10539, 320, 4498, 264, 3347, 29599, 476, 52635, 32187, 374, 389, 279, 2115, 3108, 315, 697, 9115, 1815, 11, 498, 1265, 5648, 432, 311, 279, 1290, 1326, 16, 16, 4962, 320, 1986, 1917, 3363, 279, 1882, 553, 892, 264, 7310, 14177, 973, 504, 264, 2936, 892, 374, 537, 279, 1852, 438, 151645, 198, 151644, 77091, 198, 515, 262, 330, 60565, 82, 788, 330, 28715, 389, 279, 1482, 9442, 5601, 323, 279, 36274, 7310, 594, 2639, 11, 432, 4977, 1075, 279, 1850, 3308, 315, 1917, 374, 311, 2506, 2087, 7678, 518, 264, 6783, 4628, 10346, 262, 330, 53103, 13009, 788, 10239, 262, 330, 1311, 2019, 788, 2278, 286, 5212, 606, 788, 330, 13096, 25515, 287, 2895, 7386, 497, 330, 17119, 367, 788, 330, 785, 38193, 7310, 686, 3060, 2087, 7678, 518, 264, 6783, 4628, 4092, 311, 279, 12909, 1815, 92181, 286, 5212, 606, 788, 330, 73648, 25515, 2895, 7386, 497, 330, 17119, 367, 788, 330, 2679, 279, 5795, 374, 311, 728, 7678, 518, 458, 19246, 11, 279, 7310, 686, 10354, 1526, 279, 19246, 92181, 286, 5212, 606, 788, 330, 412, 9579, 497, 330, 17119, 367, 788, 330, 2679, 279, 5795, 374, 311, 2484, 2115, 11, 279, 7310, 686, 6301, 1495, 323, 1281, 264, 2115, 2484, 518, 279, 8311, 882, 92181, 286, 5212, 606, 788, 330, 412, 10539, 497, 330, 17119, 367, 788, 330, 2679, 279, 5795, 374, 311, 2484, 1290, 11, 279, 7310, 686, 6301, 1495, 323, 1281, 264, 1290, 2484, 518, 279, 8311, 882, 92181, 286, 5212, 606, 788, 330, 43888, 58668, 497, 330, 17119, 367, 788, 330, 2679, 279, 5795, 374, 311, 42780, 11, 279, 7310, 686, 5263, 1181, 4628, 27079, 92181, 286, 5212, 606, 788, 330, 450, 3672, 58668, 497, 330, 17119, 367, 788, 330, 2679, 279, 5795, 374, 311, 1622, 7865, 349, 11, 279, 7310, 686, 6301, 1495, 27079, 92181, 286, 5212, 606, 788, 330, 37847, 15947, 9579, 497, 330, 17119, 367, 788, 330, 2679, 5871, 11, 279, 7310, 686, 2297, 33025, 311, 279, 2115, 304, 1973, 311, 5545, 279, 1790, 6821, 389, 279, 6021, 11, 1492, 2441, 7310, 2297, 33025, 11, 5648, 264, 6301, 7218, 7310, 11, 476, 2506, 311, 279, 1290, 518, 3347, 24722, 92181, 286, 5212, 606, 788, 330, 37847, 15947, 10539, 497, 330, 17119, 367, 788, 330, 2679, 5871, 11, 279, 7310, 686, 2297, 33025, 311, 279, 1290, 304, 1973, 311, 5545, 279, 1790, 6821, 389, 279, 6021, 11, 1492, 2441, 7310, 2297, 33025, 11, 5648, 264, 6301, 7218, 7310, 11, 476, 2506, 311, 279, 1290, 518, 3347, 24722, 92181, 286, 5212, 606, 788, 330, 47856, 9579, 497, 330, 17119, 367, 788, 330, 2679, 1052, 374, 264, 3347, 29599, 476, 52635, 32187, 389, 279, 1290, 3108, 315, 279, 12909, 1815, 11, 279, 7310, 686, 5648, 432, 553, 7218, 311, 279, 2115, 92181, 286, 5212, 606, 788, 330, 47856, 10539, 497, 330, 17119, 367, 788, 330, 2679, 1052, 374, 264, 3347, 29599, 476, 52635, 32187, 389, 279, 2115, 3108, 315, 279, 12909, 1815, 11, 279, 7310, 686, 5648, 432, 553, 7218, 311, 279, 1290, 92181, 286, 5212, 606, 788, 330, 2468, 497, 330, 17119, 367, 788, 330, 785, 7310, 686, 3161, 68641, 504, 264, 2936, 11, 421, 5871, 1189, 532, 262, 5133, 92, 151645, 198]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84fb5f460004278b842cdb638622376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/26 shards):   0%|          | 0/7667416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(tokenized_dataset[0:5])\n",
    "# tokenized_dataset.save_to_disk(\"./tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "340f3fd7-a707-4596-9c42-c42b8ec295ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant<|im_end|>\\n<|im_start|>user\\n0.002 = 1000 \\n1 = x?<|im_end|>\\n<|im_start|>assistant\\nTo find the value of x, we can set up a proportion using the given information:\\n\\n0.002/1000 = 1/x\\n\\nTo solve for x, we can cross multiply:\\n\\n0.002 * x = 1000 * 1\\n\\n0.002x = 1000\\n\\nDividing both sides by 0.002:\\n\\nx = 1000 / 0.002\\n\\nx = 500,000\\n\\nTherefore, 1 is equal to 500,000 in this proportion.<|im_end|>\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.decode(tokenized_dataset[\"train\"][\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "782b76c0-a4e1-44d7-9adf-24d35a81f367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(lora_model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41bf92a4-4e12-4702-928d-d2f65a294b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f7201-fa2b-4837-85a7-50c331b1ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"Qwen1.5_MoE_lora_model_100steps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0c672b3-e252-409f-99ef-72548a3fff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 7,667,416\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 63,706,752\n",
      "/workspace/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 58:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.948700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.581200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.917700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.981200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.252900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.564800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.572700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.618600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.330100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.918300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.350300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.698600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.345800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.443800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.201900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.593000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.743000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.469300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.860800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.952900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.294300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.397100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.894600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.496800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.295100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.211900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.634900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.381100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.313400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.601100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.944500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.330400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.509900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.270300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.770200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.586400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.856400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.771900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.956900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.910200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.074700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.495200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.557300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.729700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.533400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./outputs/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9/config.json\n",
      "Model config Qwen2MoeConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2MoeForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"decoder_sparse_step\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"mlp_only_layers\": [],\n",
      "  \"model_type\": \"qwen2_moe\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts\": 60,\n",
      "  \"num_experts_per_tok\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"output_router_logits\": false,\n",
      "  \"qkv_bias\": true,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.001,\n",
      "  \"shared_expert_intermediate_size\": 5632,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./outputs/checkpoint-100/chat_template.jinja\n",
      "tokenizer config file saved in ./outputs/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./outputs/checkpoint-100/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9/config.json\n",
      "Model config Qwen2MoeConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2MoeForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"decoder_sparse_step\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"mlp_only_layers\": [],\n",
      "  \"model_type\": \"qwen2_moe\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts\": 60,\n",
      "  \"num_experts_per_tok\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"output_router_logits\": false,\n",
      "  \"qkv_bias\": true,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.001,\n",
      "  \"shared_expert_intermediate_size\": 5632,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # causal LM\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs_2\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=500,               # try a small number first\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=5,\n",
    "    bf16=True,\n",
    "\n",
    "    logging_steps=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    report_to=\"none\",      # prevents WandB from hijacking logs\n",
    "    disable_tqdm=False    # ensure progress bar is visible\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=10,\n",
    "    save_safetensors=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,          # your LoRA-wrapped model\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(\"Qwen1.5_MoE_lora_model_600steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02c8980b-6135-453c-9066-8a802bc94ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9/config.json\n",
      "Model config Qwen2MoeConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2MoeForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"decoder_sparse_step\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"mlp_only_layers\": [],\n",
      "  \"model_type\": \"qwen2_moe\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts\": 60,\n",
      "  \"num_experts_per_tok\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"output_router_logits\": false,\n",
      "  \"qkv_bias\": true,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.001,\n",
      "  \"shared_expert_intermediate_size\": 5632,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "Uploading the following files to kaaiiii/Qwen1.5b_MoE_LoRA: README.md,adapter_model.safetensors,adapter_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2699c32ef95489ba61c57ff24dd7b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675b19b2bca84006a4459bbf43d11a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kaaiiii/Qwen1.5b_MoE_LoRA/commit/dc41c10c94e28e6281b1ee7a0c9e6c164b6a59e7', commit_message='Upload model', commit_description='', oid='dc41c10c94e28e6281b1ee7a0c9e6c164b6a59e7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kaaiiii/Qwen1.5b_MoE_LoRA', endpoint='https://huggingface.co', repo_type='model', repo_id='kaaiiii/Qwen1.5b_MoE_LoRA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.push_to_hub(\"kaaiiii/Qwen1.5b_MoE_LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7853727-3f90-4d89-b443-bcc844e5f0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9/config.json\n",
      "Model config Qwen2MoeConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2MoeForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"decoder_sparse_step\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"mlp_only_layers\": [],\n",
      "  \"model_type\": \"qwen2_moe\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts\": 60,\n",
      "  \"num_experts_per_tok\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"output_router_logits\": false,\n",
      "  \"qkv_bias\": true,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.001,\n",
      "  \"shared_expert_intermediate_size\": 5632,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9/vocab.json\n",
      "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9/merges.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 42\u001b[0m\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     12\u001b[0m     model \u001b[38;5;241m=\u001b[39m lora_model,\n\u001b[1;32m     13\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m tokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen1.5_MoE_lora_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/trainer.py:2316\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2314\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   2315\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 2316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2317\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2323\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/trainer.py:2375\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2373\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 2375\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   2377\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/trainer.py:1140\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: training requires a train_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_train_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/trainer.py:1109\u001b[0m, in \u001b[0;36mTrainer._get_dataloader\u001b[0;34m(self, dataset, description, batch_size, sampler_fn, is_training, dataloader_key)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterableDataset):\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sampler_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1109\u001b[0m         dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msampler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_drop_last\n\u001b[1;32m   1111\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefetch_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_prefetch_factor\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/trainer.py:1072\u001b[0m, in \u001b[0;36mTrainer._get_train_sampler\u001b[0;34m(self, train_dataset)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m     model_input_name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1070\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m     )\n\u001b[0;32m-> 1072\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLengthGroupedSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RandomSampler(train_dataset)\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:640\u001b[0m, in \u001b[0;36mLengthGroupedSampler.__init__\u001b[0;34m(self, batch_size, dataset, lengths, model_input_name, generator)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mdict\u001b[39m, BatchEncoding)) \u001b[38;5;129;01mor\u001b[39;00m model_input_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    637\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only automatically infer lengths for datasets whose items are dictionaries with an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[0;32m--> 640\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(feature[model_input_name]) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lengths, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    642\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf lengths is a torch.Tensor, LengthGroupedSampler will be slow. Converting lengths to list[int]...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    644\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:640\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mdict\u001b[39m, BatchEncoding)) \u001b[38;5;129;01mor\u001b[39;00m model_input_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    637\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only automatically infer lengths for datasets whose items are dictionaries with an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[0;32m--> 640\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(feature[model_input_name]) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lengths, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    642\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf lengths is a torch.Tensor, LengthGroupedSampler will be slow. Converting lengths to list[int]...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    644\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2483\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_subtable\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[1;32m   2482\u001b[0m             pa_subtable_ex \u001b[38;5;241m=\u001b[39m pa_subtable\u001b[38;5;241m.\u001b[39mslice(i, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2483\u001b[0m             formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpa_subtable_ex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2485\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2486\u001b[0m \u001b[43m                \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2487\u001b[0m \u001b[43m                \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[43m                \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_all_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2490\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m formatted_output\n\u001b[1;32m   2491\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:658\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    656\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:411\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 411\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:459\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 459\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_row(row)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m/workspace/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:144\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unnest(\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from trl import SFTConfig\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# from transformers import logging as hf_logging\n",
    "# hf_logging.set_verbosity_info()\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B\", use_fast=True)\n",
    "# print(lora_model.device)\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model = lora_model,\n",
    "#     train_dataset = tokenized_dataset[\"train\"],\n",
    "#     # processing_class = tokenizer,\n",
    "#     args = SFTConfig(\n",
    "#         dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "#         per_device_train_batch_size = 8,\n",
    "#         gradient_accumulation_steps = 1,\n",
    "#         packing = True,\n",
    "#         group_by_length = True,\n",
    "#         warmup_steps = 5,\n",
    "#         bf16 = True,\n",
    "#         max_steps=500,\n",
    "#         learning_rate = 1e-4,\n",
    "#         optim = \"adamw_8bit\",\n",
    "#         weight_decay = 0.03,\n",
    "#         lr_scheduler_type = \"cosine\",\n",
    "#         seed = 3407,\n",
    "#         eval_strategy=\"no\",\n",
    "#         do_eval=False,\n",
    "#         output_dir = \"./outputs\",\n",
    "#         push_to_hub=True,\n",
    "#         remove_unused_columns=False,\n",
    "#         logging_steps = 5,\n",
    "#         disable_tqdm=False,\n",
    "#         report_to=\"none\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# trainer.model.save_pretrained(\"Qwen1.5_MoE_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94fc05b-37d5-4813-a0f1-c1b668bbd0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
