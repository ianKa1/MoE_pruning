{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5891121-b549-420a-9427-668aec7ea240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import MixtralForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# GPU and cache settings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4bc6ea7-19a1-452e-bd4f-01968fe606ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty the cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702c3569-b27a-4512-b2da-d7b1ccdde758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_disk_space(path, required_gb=100):\n",
    "\t\"\"\"Check if there's enough disk space available.\"\"\"\n",
    "\tstats = shutil.disk_usage(path)\n",
    "\tavailable_gb = stats.free / (2**30)  # Convert to GB\n",
    "\treturn available_gb >= required_gb, available_gb\n",
    "\n",
    "class PrunedMixtralSparseMoeBlock(nn.Module):\n",
    "\tdef __init__(self, original_moe, pruned_experts):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.pruned_experts = sorted(pruned_experts)\n",
    "\t\t\n",
    "\t\t# Prune the gate network\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toriginal_weight = original_moe.gate.weight\n",
    "\t\t\tmask = torch.ones(original_weight.size(0), dtype=torch.bool)\n",
    "\t\t\tmask[pruned_experts] = False\n",
    "\t\t\tnew_weight = original_weight[mask]\n",
    "\n",
    "\t\tself.gate = nn.Linear(original_moe.gate.in_features, original_moe.gate.out_features - len(pruned_experts), bias=False)\n",
    "\t\tself.gate = self.gate.to(original_weight.device)\n",
    "\t\tself.gate.weight.data = new_weight\n",
    "\t\t\n",
    "\t\t# Actually remove the pruned experts\n",
    "\t\tself.experts = nn.ModuleList([expert for i, expert in enumerate(original_moe.experts) if i not in pruned_experts])\n",
    "\t\tself.num_experts = len(self.experts)\n",
    "\n",
    "\tdef forward(self, hidden_states):\n",
    "\t\tgate_logits = self.gate(hidden_states)\n",
    "\t\tweights, selected_experts = torch.topk(gate_logits, k=2, dim=-1)\n",
    "\t\tweights = nn.functional.softmax(weights, dim=-1)\n",
    "\t\t\n",
    "\t\thidden_states = hidden_states.unsqueeze(1)  # Add sequence length dimension\n",
    "\t\texpert_outputs = torch.zeros_like(hidden_states)\n",
    "\t\tfor i, expert in enumerate(self.experts):\n",
    "\t\t\texpert_mask = (selected_experts == i).any(dim=-1).unsqueeze(-1)\n",
    "\t\t\texpert_inputs = hidden_states * expert_mask\n",
    "\t\t\texpert_outputs += expert(expert_inputs) * expert_mask\n",
    "\t\t\n",
    "\t\thidden_states = hidden_states.squeeze(1)  # Remove sequence length dimension\n",
    "\t\texpert_outputs = expert_outputs.squeeze(1)\n",
    "\t\t\n",
    "\t\toutput = torch.einsum(\"...e,...ec->...c\", weights, expert_outputs)\n",
    "\t\treturn output\n",
    "\n",
    "def prune_mixtral_experts(model, pruned_experts_per_layer):\n",
    "\tfor layer_idx, pruned_experts in pruned_experts_per_layer.items():\n",
    "\t\toriginal_moe = model.model.layers[layer_idx].block_sparse_moe\n",
    "\t\tpruned_moe = PrunedMixtralSparseMoeBlock(original_moe, pruned_experts)\n",
    "\t\tmodel.model.layers[layer_idx].block_sparse_moe = pruned_moe\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ecd2ae-cd37-4511-a529-203c6946a693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Only 31.10GB available in /root/.cache/huggingface\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue anyway? (y/n):  y\n"
     ]
    }
   ],
   "source": [
    "# Check disk space in the HF cache directory\n",
    "# cache_dir = \"/root/.cache/huggingface\"\n",
    "cache_dir = \"/root/.cache/huggingface\"\n",
    "has_space, available_gb = check_disk_space(cache_dir)\n",
    "\n",
    "if not has_space:\n",
    "    print(f\"Warning: Only {available_gb:.2f}GB available in {cache_dir}\")\n",
    "    response = input(\"Continue anyway? (y/n): \")\n",
    "    if response.lower() != 'y':\n",
    "        print(\"Aborting operation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c57c787-0d43-489d-87c3-bb9e32c3fe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61a7d2d5d07479e8f1443570f32e5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create temporary save directory\n",
    "temp_save_dir = Path(\"./temp_model_save\")\n",
    "temp_save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = MixtralForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    torch_dtype=torch.float16,  # Use half precision\n",
    "    device_map=\"auto\"  # Automatically handle multi-GPU\n",
    "    # device_map=None,\n",
    "    # low_cpu_mem_usage=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2fa237b-f924-4be8-8415-f5f78fc510a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning the model...\n",
      "Model pruned!\n",
      "Saving model to temporary directory: temp_model_save\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('temp_model_save/tokenizer_config.json',\n",
       " 'temp_model_save/special_tokens_map.json',\n",
       " 'temp_model_save/chat_template.jinja',\n",
       " 'temp_model_save/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_experts_per_layer = {0: [1, 0, 5, 2], 1: [7, 0, 2, 5], 2: [0, 1, 5, 4], 3: [2, 3, 1, 4], 4: [2, 0, 7, 6], 5: [7, 2, 4, 3], 6: [7, 3, 4, 2], 7: [4, 7, 3, 5], 8: [6, 2, 1, 5], 9: [7, 6, 0, 2], 10: [4, 0, 1, 3], 11: [4, 7, 6, 3], 12: [0, 3, 5, 1], 13: [2, 7, 5, 1], 14: [3, 1, 4, 6], 15: [3, 6, 5, 4], 16: [5, 0, 3, 7], 17: [5, 3, 1, 4], 18: [5, 1, 6, 3], 19: [4, 3, 5, 1], 20: [3, 7, 4, 5], 21: [7, 5, 4, 2], 22: [3, 2, 6, 5], 23: [4, 5, 6, 3], 24: [5, 6, 2, 0], 25: [3, 4, 5, 1], 26: [2, 0, 3, 4], 27: [7, 6, 5, 2], 28: [4, 2, 1, 7], 29: [3, 6, 4, 0], 30: [4, 1, 7, 2], 31: [5, 0, 3, 1]}\n",
    "\n",
    "print(\"Pruning the model...\")\n",
    "pruned_model = prune_mixtral_experts(model, pruned_experts_per_layer)\n",
    "print(\"Model pruned!\")\n",
    "pruned_model = pruned_model.to(\"cuda\")\n",
    "\n",
    "# Save in chunks first\n",
    "print(f\"Saving model to temporary directory: {temp_save_dir}\")\n",
    "pruned_model.save_pretrained(\n",
    "    temp_save_dir,\n",
    "    max_shard_size=\"2GB\",\n",
    "    safe_serialization=True\n",
    ")\n",
    "tokenizer.save_pretrained(temp_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0de3af67-14cf-40e4-b74d-44e97a37f73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing to Hugging Face Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6ef5268a1f4af49854476fe30eb60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c039acd4837e485f87afb95b58559efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a29c963820a4b2eb29ec12c72a388dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lu/model-00009-of-00025.safetensors:   4%|3         | 75.5MB / 1.96GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444b56730f4c41489285c7f244270640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lu/model-00001-of-00025.safetensors:   0%|          | 4.78MB / 1.96GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a212f30d3b94bbbbb9d9442d4530fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lu/model-00005-of-00025.safetensors:   3%|2         | 50.2MB / 1.96GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8788d2b992341408e103af532440786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lu/model-00006-of-00025.safetensors:   0%|          | 3.59MB / 1.96GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748947da8a824863831609f91477f5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lu/model-00008-of-00025.safetensors:   3%|3         | 58.7MB / 1.90GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f98995c7c14c29905b9b41d9cdd10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lu/model-00025-of-00025.safetensors:   1%|          | 8.34MB / 1.44GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3082097678224f2b993d464269ed6acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lu/model-00007-of-00025.safetensors:   2%|2         | 46.7MB / 2.00GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23cfcdda3ce4929a2ac40a816845cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lu/model-00004-of-00025.safetensors:   2%|2         | 39.1MB / 1.93GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e984ebc0b1465fab3128fdeb9a8dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lu/model-00012-of-00025.safetensors:   0%|          |  667kB / 1.96GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bb0494282446fd95185488ebb65689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lu/model-00014-of-00025.safetensors:   1%|          | 16.7MB / 1.93GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1bb1f1bf074d6a93ac323183a8229e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed to Hub!\n"
     ]
    }
   ],
   "source": [
    "# Push to Hub\n",
    "print(\"Pushing to Hugging Face Hub...\")\n",
    "pruned_model.push_to_hub(\n",
    "    \"xinyiluo448/Mixtral-8x7B-v0.1-instruct-pruned-4-experts\",\n",
    "    max_shard_size=\"2GB\",\n",
    "    safe_serialization=True\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    \"xinyiluo448/Mixtral-8x7B-v0.1-instruct-pruned-4-experts\"\n",
    ")\n",
    "print(\"Successfully pushed to Hub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0658da7e-493b-42c5-a5af-601050d966c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "print(\"Cleaning up...\")\n",
    "if temp_save_dir.exists():\n",
    "    shutil.rmtree(temp_save_dir)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c353941-67f4-49c7-b9ab-562db07efe65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
