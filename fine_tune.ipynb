{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd61476a-f805-4821-b4c4-397d8d48bfd3",
      "metadata": {
        "collapsed": true,
        "id": "bd61476a-f805-4821-b4c4-397d8d48bfd3",
        "outputId": "8aa6a079-3d2b-44a4-bc42-ef779c3a7fa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting peft\n",
            "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting trl\n",
            "  Downloading trl-0.25.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /venv/main/lib/python3.10/site-packages (from transformers) (0.34.4)\n",
            "Collecting numpy>=1.17 (from transformers)\n",
            "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from peft) (7.0.0)\n",
            "Collecting torch>=1.13.0 (from peft)\n",
            "  Downloading torch-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting datasets>=3.0.0 (from trl)\n",
            "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting sympy>=1.13.3 (from torch>=1.13.0->peft)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx>=2.5.1 (from torch>=1.13.0->peft)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch>=1.13.0->peft)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch>=1.13.0->peft)\n",
            "  Downloading triton-3.5.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting pyarrow>=21.0.0 (from datasets>=3.0.0->trl)\n",
            "  Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets>=3.0.0->trl)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets>=3.0.0->trl)\n",
            "  Downloading pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "Collecting httpx<1.0.0 (from datasets>=3.0.0->trl)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting xxhash (from datasets>=3.0.0->trl)\n",
            "  Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.19 (from datasets>=3.0.0->trl)\n",
            "  Downloading multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting anyio (from httpx<1.0.0->datasets>=3.0.0->trl)\n",
            "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: certifi in /venv/main/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (2025.8.3)\n",
            "Collecting httpcore==1.* (from httpx<1.0.0->datasets>=3.0.0->trl)\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: idna in /venv/main/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (3.10)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl)\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
            "  Downloading yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.13.0->peft)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /venv/main/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl) (1.3.0)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.13.0->peft)\n",
            "  Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.10/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets>=3.0.0->trl)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets>=3.0.0->trl)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m131.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m185.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
            "Downloading trl-0.25.1-py3-none-any.whl (465 kB)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m200.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl (899.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m169.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m186.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m202.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m184.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m163.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m202.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m230.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m198.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m201.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m190.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m199.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m195.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m202.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m202.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.5.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.3/170.3 MB\u001b[0m \u001b[31m156.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading multiprocess-0.70.18-py310-none-any.whl (134 kB)\n",
            "Downloading aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m278.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
            "Downloading yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "Downloading frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (219 kB)\n",
            "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m232.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m184.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\n",
            "Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (47.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m176.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m791.7/791.7 kB\u001b[0m \u001b[31m147.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m192.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (20 kB)\n",
            "Downloading pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m175.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
            "Installing collected packages: pytz, nvidia-cusparselt-cu12, mpmath, xxhash, tzdata, triton, sympy, safetensors, regex, pyarrow, propcache, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, MarkupSafe, h11, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, jinja2, httpcore, anyio, aiosignal, tokenizers, nvidia-cusolver-cu12, httpx, aiohttp, transformers, torch, datasets, bitsandbytes, accelerate, trl, peft\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52/52\u001b[0m [peft]0m [peft]0m [trl]lerate]s]er-cu12]2]\n",
            "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 accelerate-1.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.12.0 async-timeout-5.0.1 attrs-25.4.0 bitsandbytes-0.48.2 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jinja2-3.1.6 mpmath-1.3.0 multidict-6.7.0 multiprocess-0.70.18 networkx-3.4.2 numpy-2.2.6 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 pandas-2.3.3 peft-0.18.0 propcache-0.4.1 pyarrow-22.0.0 pytz-2025.2 regex-2025.11.3 safetensors-0.7.0 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.1 transformers-4.57.3 triton-3.5.1 trl-0.25.1 tzdata-2025.2 xxhash-3.6.0 yarl-1.22.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "pip install transformers peft accelerate trl bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39317def-ae85-400f-9f77-081c2a0fa5ee",
      "metadata": {
        "collapsed": true,
        "id": "39317def-ae85-400f-9f77-081c2a0fa5ee",
        "outputId": "a99f6e2c-0ae6-49cd-91fe-d1703bae7687"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /venv/main/lib/python3.10/site-packages (from seaborn) (2.2.6)\n",
            "Requirement already satisfied: pandas>=1.2 in /venv/main/lib/python3.10/site-packages (from seaborn) (2.3.3)\n",
            "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
            "  Downloading matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading fonttools-4.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (113 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Collecting pillow>=8 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
            "Collecting pyparsing>=3 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /venv/main/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "Downloading matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m160.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m153.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m162.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [seaborn]m6/8\u001b[0m [matplotlib]\n",
            "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.61.0 kiwisolver-1.4.9 matplotlib-3.10.7 pillow-12.0.0 pyparsing-3.2.5 seaborn-0.13.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fa41d51-b645-440d-9145-92dafbd0fbff",
      "metadata": {
        "id": "1fa41d51-b645-440d-9145-92dafbd0fbff"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import IA3Config, get_peft_model, TaskType\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "433f90d5-56f0-47f2-80c9-2eda7575833e",
      "metadata": {
        "id": "433f90d5-56f0-47f2-80c9-2eda7575833e"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63daa923-183d-48a5-a80e-1d3e8faa1b16",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "34259f8c6ffd4c0ab5b50432b72d7bb4"
          ]
        },
        "id": "63daa923-183d-48a5-a80e-1d3e8faa1b16",
        "outputId": "899231f2-2061-4c58-91a7-41e897a3b29b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34259f8c6ffd4c0ab5b50432b72d7bb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a815e1a8-2ff7-48f2-9fcf-3c8c04dd5460",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f441f7d6f8f74fd193fb0c74467aec23",
            "362bae7a3ec74d01acfbb815fad2d2a6",
            "949a18a9c5e941afae3bd64babdfe832",
            "a1aa36cc1227475e97e37b2c59c488a0",
            "e62b4c03fe824a85a016ac349f1904c1",
            "f5f2ea20d0034284943d97556b244e45",
            "8547119fc7a948d28a2488f03ac60121",
            "0f97da9cd7e5415c9290d5fa76146d24",
            "818174448b734f6ebc61c83b34450110",
            "e1df7e6bf5f04bdbbe49da560053944f",
            "03ac70df23624534a54b21302bb0e825",
            "5fac27869cd24624a80dc07869a5d5dc",
            "61176a79719244ef80c661c99f6d470d",
            "513d66b834c649afaeab3696d490528b",
            "dd339e59b4774c69bbbf146ed6afa855",
            "dbdef2d8130b4f69a554d678893d20c9",
            "ed39e0a9ec6645eab2f644d94102eca3",
            "ad24442ea448439286c4eda6f63d39af",
            "85750690f3ef49c0bde3d53ef43c4073",
            "09f5097943104900bd4445b1c4ebf3b2",
            "8f6f0b3acc9d42a09d336ff29d28bac5",
            "f990df2d0c0246b080cec175ae709c99",
            "ad1991279b3e4fb999c8c6104412fd64",
            "b2c8442f395741f389c9fdbac56fe52c",
            "d11170836f254ce2bf123f39893526dd",
            "c621eb5709324988a803329b32400360",
            "6bf17670af894cd9a20a50ba958e8a23",
            "c53dcddd74104e7fa4fbb3e065522ece"
          ]
        },
        "id": "a815e1a8-2ff7-48f2-9fcf-3c8c04dd5460",
        "outputId": "7999e886-caff-406e-bb99-288085c784c2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f441f7d6f8f74fd193fb0c74467aec23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "362bae7a3ec74d01acfbb815fad2d2a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "949a18a9c5e941afae3bd64babdfe832",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1aa36cc1227475e97e37b2c59c488a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e62b4c03fe824a85a016ac349f1904c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5f2ea20d0034284943d97556b244e45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8547119fc7a948d28a2488f03ac60121",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f97da9cd7e5415c9290d5fa76146d24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "818174448b734f6ebc61c83b34450110",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1df7e6bf5f04bdbbe49da560053944f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00005-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03ac70df23624534a54b21302bb0e825",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00007-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fac27869cd24624a80dc07869a5d5dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61176a79719244ef80c661c99f6d470d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00006-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "513d66b834c649afaeab3696d490528b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd339e59b4774c69bbbf146ed6afa855",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00008-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbdef2d8130b4f69a554d678893d20c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00009-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed39e0a9ec6645eab2f644d94102eca3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00010-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad24442ea448439286c4eda6f63d39af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00011-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85750690f3ef49c0bde3d53ef43c4073",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00012-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09f5097943104900bd4445b1c4ebf3b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00013-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f6f0b3acc9d42a09d336ff29d28bac5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00014-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f990df2d0c0246b080cec175ae709c99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00015-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad1991279b3e4fb999c8c6104412fd64",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00016-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2c8442f395741f389c9fdbac56fe52c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00017-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d11170836f254ce2bf123f39893526dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00018-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c621eb5709324988a803329b32400360",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00019-of-00019.safetensors:   0%|          | 0.00/4.22G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bf17670af894cd9a20a50ba958e8a23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c53dcddd74104e7fa4fbb3e065522ece",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 15,732,736 || all params: 46,718,525,440 || trainable%: 0.0337\n"
          ]
        }
      ],
      "source": [
        "quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", truncation=True, padding=True, padding_side=\"right\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", quantization_config=quantization_config)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(r = 16,\n",
        "                    lora_alpha=16,\n",
        "                    target_modules = [\"gate\", \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    lora_dropout=0.1\n",
        "                    )\n",
        "\n",
        "\n",
        "\n",
        "lora_model = get_peft_model(model, config)\n",
        "\n",
        "lora_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d6e25a0-32eb-4bc6-9dae-76cc2c733a8d",
      "metadata": {
        "scrolled": true,
        "collapsed": true,
        "id": "0d6e25a0-32eb-4bc6-9dae-76cc2c733a8d",
        "outputId": "9dd4b38f-88ef-41d3-a9ed-a6d61dbe0a97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name: base_model.model.model.layers.0.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.1.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.2.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.3.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.4.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.5.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.6.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.7.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.8.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.9.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.10.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.11.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.12.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.13.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.14.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.15.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.16.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.17.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.18.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.19.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.20.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.21.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.22.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.23.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.24.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.25.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.26.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.27.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.28.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.29.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.30.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n",
            "name: base_model.model.model.layers.31.block_sparse_moe.gate module: lora.Linear4bit(\n",
            "  (base_layer): Linear4bit(in_features=4096, out_features=8, bias=False)\n",
            "  (lora_dropout): ModuleDict(\n",
            "    (default): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lora_A): ModuleDict(\n",
            "    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "  )\n",
            "  (lora_B): ModuleDict(\n",
            "    (default): Linear(in_features=16, out_features=8, bias=False)\n",
            "  )\n",
            "  (lora_embedding_A): ParameterDict()\n",
            "  (lora_embedding_B): ParameterDict()\n",
            "  (lora_magnitude_vector): ModuleDict()\n",
            ")\n",
            "weights torch.Size([16384, 1])\n",
            "weight after dequantization torch.Size([8, 4096])\n"
          ]
        }
      ],
      "source": [
        "import bitsandbytes as bnb\n",
        "\n",
        "def get_gate_weight_4bit(gate):\n",
        "    # gate: Linear4bit(in_features=2048, out_features=60, bias=False)\n",
        "\n",
        "    if hasattr(gate, \"base_layer\"):\n",
        "        gate = gate.base_layer\n",
        "\n",
        "    qweight = gate.weight.data\n",
        "    qweight = qweight.view(-1)\n",
        "\n",
        "    W = bnb.functional.dequantize_4bit(\n",
        "        qweight,\n",
        "        quant_state=gate.quant_state,\n",
        "        quant_type=\"nf4\",\n",
        "    )\n",
        "    W = W.view(gate.out_features, gate.in_features)\n",
        "    return W  # float16/float32 matrix, shape == (60, 2048)\n",
        "\n",
        "def get_router_l2_norms(model):\n",
        "    layer_norms = {}\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if name.endswith(\"block_sparse_moe.gate\"):\n",
        "            print(f\"name: {name} module: {module}\")\n",
        "            weight = module.weight\n",
        "            print(f\"weights {weight.shape}\")\n",
        "            weight_dequantization = get_gate_weight_4bit(module)\n",
        "            print(f\"weight after dequantization {weight_dequantization.shape}\")\n",
        "           # norms = torch.norm(weight_dequantization.float(), dim=1)  # shape (60,)\n",
        "            #print(f\" norms {norms.shape}\")\n",
        "            #layer_norms[name] = norms.detach().cpu()\n",
        "\n",
        "\n",
        "\n",
        "            # Case 2: normal FP16/FP32 Linear router\n",
        "\n",
        "            # Compute per-expert L2 norms\n",
        "            #norms = torch.norm(W.float(), dim=1)  # shape (8,)\n",
        "            #print(f\"Router L2 norms: {norms.tolist()}\")\n",
        "\n",
        "            #layer_norms[name] = norms.detach().cpu()\n",
        "\n",
        "    return layer_norms\n",
        "\n",
        "\n",
        "norms = get_router_l2_norms(lora_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f32c38cd-c07b-47e0-b9c9-b1aec175634e",
      "metadata": {
        "id": "f32c38cd-c07b-47e0-b9c9-b1aec175634e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_router_modules(model):\n",
        "    routers = {}  # { layer_index: gate_module }\n",
        "    for name, module in model.named_modules():\n",
        "        if name.endswith(\"block_sparse_moe.gate\"):\n",
        "            layer = int(re.search(r\"layers\\.(\\d+)\\.\", name).group(1))\n",
        "            routers[layer] = module\n",
        "    return routers\n",
        "\n",
        "\n",
        "def get_W_from_gate(gate):\n",
        "    # unwrap lora\n",
        "    if hasattr(gate, \"base_layer\"):\n",
        "        gate = gate.base_layer\n",
        "\n",
        "    # quantized case\n",
        "    if hasattr(gate, \"quant_state\"):\n",
        "        q = gate.weight.data.view(-1)\n",
        "        W = bnb.functional.dequantize_4bit(\n",
        "            q,\n",
        "            quant_state=gate.quant_state,\n",
        "            quant_type=\"nf4\"\n",
        "        ).view(gate.out_features, gate.in_features)\n",
        "    else:\n",
        "        W = gate.weight.data\n",
        "\n",
        "    return W.float()   # (8, 4096)\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# 2. Extract LoRA A and B for all router layers\n",
        "# --------------------------\n",
        "def extract_router_lora_AB(model):\n",
        "    A_dict, B_dict = {}, {}\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"block_sparse_moe.gate.lora_A\" in name:\n",
        "            layer = int(re.search(r\"layers\\.(\\d+)\\.\", name).group(1))\n",
        "            A_dict[layer] = param.detach().float()\n",
        "        elif \"block_sparse_moe.gate.lora_B\" in name:\n",
        "            layer = int(re.search(r\"layers\\.(\\d+)\\.\", name).group(1))\n",
        "            B_dict[layer] = param.detach().float()\n",
        "\n",
        "    return A_dict, B_dict\n",
        "\n",
        "# --------------------------\n",
        "# 3. Compute W, AB, and W+AB norms\n",
        "# --------------------------\n",
        "def compute_router_norms(model, alpha=4, r=4):\n",
        "    scale = alpha / r\n",
        "\n",
        "    routers = extract_router_modules(model)\n",
        "    A_dict, B_dict = extract_router_lora_AB(model)\n",
        "\n",
        "    norms_W = {}\n",
        "    norms_AB = {}\n",
        "    norms_WAB = {}\n",
        "\n",
        "    for layer, gate in routers.items():\n",
        "\n",
        "        # ----- W -----\n",
        "        W = get_W_from_gate(gate)        # (8, 4096)\n",
        "        norms_W[layer] = torch.norm(W, dim=1).tolist()\n",
        "\n",
        "        # ----- AB -----\n",
        "        if layer in A_dict:   # LoRA exists\n",
        "            A = A_dict[layer]     # (rank, 4096)\n",
        "            B = B_dict[layer]     # (8, rank)\n",
        "            delta = scale * (B @ A)  # (8, 4096)\n",
        "        else:\n",
        "            delta = torch.zeros_like(W)\n",
        "\n",
        "        norms_AB[layer] = torch.norm(delta, dim=1).tolist()\n",
        "\n",
        "        # ----- W + AB -----\n",
        "        merged = W + delta\n",
        "        norms_WAB[layer] = torch.norm(merged, dim=1).tolist()\n",
        "\n",
        "    return norms_W, norms_AB, norms_WAB\n",
        "\n",
        "norm_W, norm_AB, norm_WAB = compute_router_norms(lora_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37db660a-09f2-4a5e-9f63-c62d1b736c85",
      "metadata": {
        "id": "37db660a-09f2-4a5e-9f63-c62d1b736c85"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "import logging\n",
        "logging.getLogger(\"trl.trainer.sft_trainer\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a96bfe3a-e8e5-478f-bb5c-65e1f69ad56e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3997652045284d5fae894ab247329744",
            "575fbc8ef1b748b8984c102690febe61",
            "331fd5e922f44293a13ae257c5176113",
            "623248312d1a4e66a798caa6a381d6c8",
            "a64f277cc43b4881bf2753e814cf8c9a",
            "4d64f09730f24cff84b598a926cd77b0",
            "3619961c4b57486f9e4ce4c8ee306bd1",
            "d83c776c32e143038d0977160ae50763",
            "bccfe07c1fb24fe28f6b5fe9a68b45ee",
            "056c961e89a7447cbc2e28aad33ba3fc",
            "96088a72ae424ed2924b2757eb57c164",
            "d6751a821dd3447386d74aa07dd0e857",
            "8d6fd2a324a34a5b99865cd31337514c",
            "6a12eeb2af8a495991a86c1ef1d00d79",
            "7f87eaa11e5a49f8b3328b2e1713a027",
            "de448b220e0d4bd49d6a38e49ca240a0",
            "584d64c29100445c8ea28f60ccf14505",
            "7dbe8a9bb19f4de2aeafaf505413acb9",
            "0a4f8c3fd3854579953a73d87b61766c",
            "40b7716f18384762ae4475ca4955c3bc",
            "80814f50ccb24c48b6c5ea41f50dbc56",
            "39eaef1225684d84954d6232f9e7352f",
            "d5ca9449d0c34c139ca105b5ad2764ac",
            "39497a4cde9b415db66562ba00064423",
            "b71b06049ddf4bd4bab6770f50ee86ab",
            "095e8a04cb034bfca2d1988695f300b2",
            "b48becaba3d440f4a86706ed94b97800",
            "9cc57a40e0764d458c04398a6f17fdc4"
          ]
        },
        "id": "a96bfe3a-e8e5-478f-bb5c-65e1f69ad56e",
        "outputId": "f5d3ef13-700d-48f3-f0fa-67c77394ad6f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3997652045284d5fae894ab247329744",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/344 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "575fbc8ef1b748b8984c102690febe61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "331fd5e922f44293a13ae257c5176113",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "623248312d1a4e66a798caa6a381d6c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0/22 [00:00<?, ?files/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a64f277cc43b4881bf2753e814cf8c9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00022.parquet:   0%|          | 0.00/332M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d64f09730f24cff84b598a926cd77b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00001-of-00022.parquet:   0%|          | 0.00/323M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3619961c4b57486f9e4ce4c8ee306bd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00002-of-00022.parquet:   0%|          | 0.00/183M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d83c776c32e143038d0977160ae50763",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00003-of-00022.parquet:   0%|          | 0.00/137M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bccfe07c1fb24fe28f6b5fe9a68b45ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00004-of-00022.parquet:   0%|          | 0.00/329M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "056c961e89a7447cbc2e28aad33ba3fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00005-of-00022.parquet:   0%|          | 0.00/331M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96088a72ae424ed2924b2757eb57c164",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00006-of-00022.parquet:   0%|          | 0.00/255M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6751a821dd3447386d74aa07dd0e857",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00007-of-00022.parquet:   0%|          | 0.00/256M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d6fd2a324a34a5b99865cd31337514c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00008-of-00022.parquet:   0%|          | 0.00/251M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a12eeb2af8a495991a86c1ef1d00d79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00009-of-00022.parquet:   0%|          | 0.00/316M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f87eaa11e5a49f8b3328b2e1713a027",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00010-of-00022.parquet:   0%|          | 0.00/347M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de448b220e0d4bd49d6a38e49ca240a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00011-of-00022.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "584d64c29100445c8ea28f60ccf14505",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00012-of-00022.parquet:   0%|          | 0.00/476M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dbe8a9bb19f4de2aeafaf505413acb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00013-of-00022.parquet:   0%|          | 0.00/594M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a4f8c3fd3854579953a73d87b61766c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00014-of-00022.parquet:   0%|          | 0.00/252M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40b7716f18384762ae4475ca4955c3bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00015-of-00022.parquet:   0%|          | 0.00/77.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80814f50ccb24c48b6c5ea41f50dbc56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00016-of-00022.parquet:   0%|          | 0.00/92.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39eaef1225684d84954d6232f9e7352f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00017-of-00022.parquet:   0%|          | 0.00/95.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5ca9449d0c34c139ca105b5ad2764ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00018-of-00022.parquet:   0%|          | 0.00/99.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39497a4cde9b415db66562ba00064423",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00019-of-00022.parquet:   0%|          | 0.00/119M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b71b06049ddf4bd4bab6770f50ee86ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00020-of-00022.parquet:   0%|          | 0.00/98.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "095e8a04cb034bfca2d1988695f300b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00021-of-00022.parquet:   0%|          | 0.00/109M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b48becaba3d440f4a86706ed94b97800",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/7667416 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cc57a40e0764d458c04398a6f17fdc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/22 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset(\"Na0s/sft-ready-Text-Generation-Augmented-Data\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed979bba-4a42-42eb-8c7b-6440577f78a9",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "4617ddc1854d40f0863e52c9e8237d98",
            "54ee144ca35c40fc881ca5d1f9e20651",
            "a8aa92bae6da4f5fac4166a302c0f312",
            "e3b5065592aa40898f5abd9bb6c3f298"
          ]
        },
        "id": "ed979bba-4a42-42eb-8c7b-6440577f78a9",
        "outputId": "56d56563-da91-4e15-9243-8b7a05ab9855"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4617ddc1854d40f0863e52c9e8237d98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/7667416 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54ee144ca35c40fc881ca5d1f9e20651",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/7667416 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8aa92bae6da4f5fac4166a302c0f312",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Packing train dataset:   0%|          | 0/7667416 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3b5065592aa40898f5abd9bb6c3f298",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/61 shards):   0%|          | 0/2491572 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32000}.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 10:09:03, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.887700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.810800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.776700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.849100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.737500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.835300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.788200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.790700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.749900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.718400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.775800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.774600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.776000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.799500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.729900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.751100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.815500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.772900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.787100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.650100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.730400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.772500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.744800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.806400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.765000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.760700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.764800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.734600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.742300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.788200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.709500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.701100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.720900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.791000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.735500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.775000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.767700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.759500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.783100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.711700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.789100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.717300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.765500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.783100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.664700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.708100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.705800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.720600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.773400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.684000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.705500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.707600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.768900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.761100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.701300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.712200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.735300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.744500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.783600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.743400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.721900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.688200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.776000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.790000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.719600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.722500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.758300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.708200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.746400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.755200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.702900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.710200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.728300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.766200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.741100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.739200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.700700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.728900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.786300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.699600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.723500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.718200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.796300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.772500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.661900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.706100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.779300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.772400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.747200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.772600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.728100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.787700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.716400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.782200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.715000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.769300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>0.748300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.739500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>0.741900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.707100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=0.7489152665138245, metrics={'train_runtime': 36581.9995, 'train_samples_per_second': 0.437, 'train_steps_per_second': 0.027, 'total_flos': 4.5376709815780147e+18, 'train_loss': 0.7489152665138245, 'epoch': 0.006421648661969231})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = lora_model,\n",
        "    train_dataset = dataset,\n",
        "    processing_class = tokenizer,\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 16,\n",
        "        packing = True,\n",
        "        group_by_length = True,\n",
        "        warmup_steps = 5,\n",
        "        bf16 = True,\n",
        "        max_steps=1000,\n",
        "        learning_rate = 2e-4,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        eval_strategy=\"no\",\n",
        "        do_eval=False,\n",
        "        output_dir = \"./outputs\",\n",
        "        push_to_hub=True,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        ")\n",
        "tokenized_dataset = trainer.train_dataset\n",
        "tokenized_dataset.save_to_disk(\"tokenized_packed_mixtral_dataset\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "825c2f10-556f-4b28-b960-9a492cab5a2c",
      "metadata": {
        "id": "825c2f10-556f-4b28-b960-9a492cab5a2c"
      },
      "outputs": [],
      "source": [
        "lora_model.save_pretrained(\"saved_lora_adapter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5a706ae-75e8-44a7-9885-eaaebf46fc04",
      "metadata": {
        "scrolled": true,
        "id": "f5a706ae-75e8-44a7-9885-eaaebf46fc04",
        "outputId": "e02b1dd2-e8c2-409a-ef35-bb4db375098a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: [2.29744553565979, 2.434562921524048, 2.315584659576416, 2.138937473297119, 2.306791305541992, 2.150372266769409, 2.479288339614868, 1.9992676973342896], 1: [2.0431435108184814, 2.1275172233581543, 1.99368155002594, 1.4405370950698853, 1.914389729499817, 2.099679708480835, 2.1274726390838623, 2.046830177307129], 2: [1.664521336555481, 1.5987746715545654, 1.6822432279586792, 1.6340171098709106, 1.688033103942871, 1.7258613109588623, 1.785442590713501, 1.7724609375], 3: [1.397202491760254, 1.5089452266693115, 1.5188285112380981, 1.562635064125061, 1.543931007385254, 1.5004613399505615, 1.498637080192566, 1.5126264095306396], 4: [1.206209421157837, 1.211559534072876, 1.1849654912948608, 1.2032876014709473, 1.1436771154403687, 1.2141910791397095, 1.2332735061645508, 1.1471960544586182], 5: [1.0494977235794067, 1.000311255455017, 1.0306810140609741, 1.0619970560073853, 0.9863646626472473, 1.056584358215332, 1.0974808931350708, 1.0247920751571655], 6: [0.9021797180175781, 1.001076340675354, 0.9318891763687134, 0.9151932597160339, 1.0271083116531372, 0.9496721625328064, 0.9483057856559753, 0.9483518004417419], 7: [0.9641839265823364, 0.9213380813598633, 0.9238978028297424, 0.9477779865264893, 0.9203665852546692, 0.9059997200965881, 0.9311438798904419, 0.8773630261421204], 8: [0.9352346658706665, 0.8948390483856201, 0.939812421798706, 0.8591238856315613, 0.9120206236839294, 0.8990839719772339, 0.9288962483406067, 0.8840375542640686], 9: [0.8429542183876038, 0.8651765584945679, 0.780945897102356, 0.8091170787811279, 0.8386470079421997, 0.810889720916748, 0.8618057370185852, 0.8782757520675659], 10: [0.5839974880218506, 0.5652604103088379, 0.6922385096549988, 0.5955684185028076, 0.6848217248916626, 0.5660621523857117, 0.7483538389205933, 0.6204096674919128], 11: [0.6733099222183228, 0.6323059797286987, 0.6052069067955017, 0.9123212099075317, 0.4804724156856537, 0.58506840467453, 0.5847777128219604, 0.668673038482666], 12: [0.6164461374282837, 0.6589135527610779, 0.6586150527000427, 0.6407424211502075, 0.6499696373939514, 0.5747818350791931, 0.7663763761520386, 0.9209083318710327], 13: [0.5798264741897583, 0.526418924331665, 0.6115885376930237, 0.5278785824775696, 0.43995076417922974, 0.5139283537864685, 0.5680831074714661, 0.45656532049179077], 14: [0.49213963747024536, 0.5017862319946289, 0.5531399846076965, 0.4750741124153137, 0.509533166885376, 0.5131832957267761, 0.5135865807533264, 0.5088028907775879], 15: [0.552396297454834, 0.5475438237190247, 0.5366414189338684, 0.7170367240905762, 0.5672072768211365, 0.585537850856781, 0.4854698181152344, 0.5485689640045166], 16: [0.5746760964393616, 0.5218405723571777, 0.4886454939842224, 0.5605677962303162, 0.46751296520233154, 0.48653170466423035, 0.49572306871414185, 0.4838503301143646], 17: [0.4547082781791687, 0.3700399100780487, 0.4142955541610718, 0.5039156079292297, 0.4739946126937866, 0.4611038267612457, 0.5518352389335632, 0.4981350898742676], 18: [0.44322460889816284, 0.4856012761592865, 0.4132314920425415, 0.4113749563694, 0.46399274468421936, 0.4895128905773163, 0.42303726077079773, 0.4867252707481384], 19: [0.4765145182609558, 0.5880346298217773, 0.5676486492156982, 0.5453633666038513, 0.4563612639904022, 0.5364981293678284, 0.5094031095504761, 0.4979563057422638], 20: [0.5869708061218262, 0.5991891026496887, 0.6024513840675354, 0.641576886177063, 0.5739687085151672, 0.5603703856468201, 0.627602756023407, 0.5956221222877502], 21: [0.6231327652931213, 0.5845517516136169, 0.6476854085922241, 0.6118570566177368, 0.6333426833152771, 0.6393265724182129, 0.6054804921150208, 0.6098812222480774], 22: [0.5857418179512024, 0.5759977102279663, 0.6118967533111572, 0.6414340138435364, 0.6034759283065796, 0.5913271903991699, 0.6324806809425354, 0.5523733496665955], 23: [0.5980777740478516, 0.6586791276931763, 0.6311233043670654, 0.6841964721679688, 0.6446267366409302, 0.6468559503555298, 0.6916813254356384, 0.615375280380249], 24: [0.6068524718284607, 0.5896539092063904, 0.5580732822418213, 0.5937771201133728, 0.6260489225387573, 0.6426573991775513, 0.6088964939117432, 0.6007577776908875], 25: [0.5167970061302185, 0.5320276021957397, 0.5377148389816284, 0.551041305065155, 0.6025493144989014, 0.5338358879089355, 0.5934905409812927, 0.5674989223480225], 26: [0.49440717697143555, 0.5611173510551453, 0.49245476722717285, 0.47386279702186584, 0.4802502989768982, 0.4998568594455719, 0.5920208692550659, 0.5307363867759705], 27: [0.5043977499008179, 0.47126683592796326, 0.43380776047706604, 0.5115593671798706, 0.47368550300598145, 0.4077490568161011, 0.45858943462371826, 0.47829699516296387], 28: [0.42005401849746704, 0.4008447825908661, 0.35252922773361206, 0.3954385817050934, 0.44750046730041504, 0.43426382541656494, 0.3824648857116699, 0.3793928325176239], 29: [0.3090428411960602, 0.35321030020713806, 0.360627144575119, 0.29150843620300293, 0.329751580953598, 0.3590811789035797, 0.34353363513946533, 0.3831387460231781], 30: [0.3137627840042114, 0.35533320903778076, 0.300981342792511, 0.3160770833492279, 0.28802165389060974, 0.35573795437812805, 0.40505513548851013, 0.345371276140213], 31: [0.39176514744758606, 0.3965851962566376, 0.3935055434703827, 0.49506279826164246, 0.3487289547920227, 0.38261929154396057, 0.42216062545776367, 0.37407681345939636]}\n"
          ]
        }
      ],
      "source": [
        "print(norm_WAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed794c9a-91a5-49c8-ae80-d6ec541a5469",
      "metadata": {
        "id": "ed794c9a-91a5-49c8-ae80-d6ec541a5469"
      },
      "outputs": [],
      "source": [
        "norm_W_after, norm_AB_after, norm_WAB_after = compute_router_norms(lora_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91663334-939b-4135-b681-c4c5c8c579fa",
      "metadata": {
        "id": "91663334-939b-4135-b681-c4c5c8c579fa"
      },
      "outputs": [],
      "source": [
        "def compute_norm_difference(norm_before, norm_after):\n",
        "    diff = {}\n",
        "    for layer in norm_before:\n",
        "        diff[layer] = [\n",
        "            abs(norm_after[layer][i] - norm_before[layer][i])\n",
        "            for i in range(len(norm_before[layer]))\n",
        "        ]\n",
        "    return diff\n",
        "\n",
        "def get_min_diff_indices(norm_diff):\n",
        "    min_indices = {}\n",
        "    for layer, diffs in norm_diff.items():\n",
        "        min_idx = int(torch.tensor(diffs).argmin().item())\n",
        "        min_indices[layer] = min_idx\n",
        "    return min_indices\n",
        "\n",
        "norm_diff = compute_norm_difference(norm_WAB_before, norm_WAB_after)\n",
        "min_diff_index = get_min_diff_indices(norm_diff)\n",
        "print(min_diff_index)\n",
        "\n",
        "matrix = np.zeros((32, 8))  # 32 × 8\n",
        "\n",
        "for layer, norms in norm_diff.items():\n",
        "    matrix[layer, :] = norms\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(\n",
        "    matrix,\n",
        "    cmap=\"viridis\",\n",
        "    cbar_kws={\"label\": \"L2 Norm Difference\"},\n",
        "    xticklabels=[f\"E{i}\" for i in range(0, 8)],\n",
        "    yticklabels=[f\"L{i}\" for i in range(0, 32)],\n",
        ")\n",
        "plt.xlabel(\"Router Column (Expert)\")\n",
        "plt.ylabel(\"Layer\")\n",
        "plt.title(\"L2 Norm Difference per Router Column (Expert) after Fine-Tuning, r=16, lora_alpha=16\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"heatmap_mixtral_r16.png\", dpi=300)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python3 (main venv)",
      "language": "python",
      "name": "main"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}